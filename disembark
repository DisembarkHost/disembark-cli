#!/usr/bin/env php
<?php
namespace Disembark;

class Run {

    const VERSION = '2.7.0';
    const USER_AGENT = 'Disembark/' . self::VERSION;

    private static $siteUrl;
    private static $token;
    private static $backup_token;
    
    // Default Thresholds
    private static $db_max_size = 209715200;       // 200MB
    private static $db_max_rows = 1000000;         // 1M rows
    private static $file_chunk_size = 2500;        // 2500 files
    private static $file_chunk_max_size = 524288000; // 500MB
    private static $large_file_threshold = 10485760; // 10MB
    
    // Options
    private static $session_id = null;
    private static $exclude_paths = [];
    private static $exclude_tables = [];
    private static $preview = false;
    private static $debug = false;
    private static $skip_db = false;
    private static $skip_files = false;
    private static $skip_checksums = false;
    private static $cleanup = false;
    private static $user_agent = self::USER_AGENT;
    private static $cookie = null;

    // Global Progress Tracking
    private static $global_bytes_total = 0;
    private static $global_bytes_processed = 0;

    /**
     * Parses CLI arguments to populate static class properties.
     * Handles flags, values, and the optional sync folder argument.
     * @return string|null The folder path if found (for sync), otherwise null.
     */
    private static function parseArguments($argv, $startIndex = 3) {
        $folder = null;
        $argc = count($argv);

        for ($i = $startIndex; $i < $argc; $i++) {
            $arg = $argv[$i];

            // --- 1. Boolean Flags ---
            if ($arg === '--preview') {
                self::$preview = true;
                continue;
            }
            if ($arg === '--debug') {
                self::$debug = true;
                continue;
            }
            if ($arg === '--skip-db') {
                self::$skip_db = true;
                continue;
            }
            if ($arg === '--skip-files') {
                self::$skip_files = true;
                continue;
            }
            if ($arg === '--skip-checksums') {
                self::$skip_checksums = true;
                continue;
            }
            if ($arg === '--cleanup') {
                self::$cleanup = true;
                continue;
            }

            // --- 2. Value Flags ---
            if (strpos($arg, '--token=') === 0) {
                self::$token = substr($arg, strlen('--token='));
                continue;
            }
            if (strpos($arg, '--session-id=') === 0) {
                self::$session_id = substr($arg, strlen('--session-id='));
                continue;
            }
            if (strpos($arg, '--db-max-size=') === 0) {
                self::$db_max_size = self::convert_to_bytes(substr($arg, strlen('--db-max-size=')));
                continue;
            }
            if (strpos($arg, '--db-max-rows=') === 0) {
                self::$db_max_rows = (int)substr($arg, strlen('--db-max-rows='));
                continue;
            }
            if (strpos($arg, '--file-chunk-size=') === 0) {
                self::$file_chunk_size = (int)substr($arg, strlen('--file-chunk-size='));
                continue;
            }
            if (strpos($arg, '--file-chunk-max-size=') === 0) {
                self::$file_chunk_max_size = self::convert_to_bytes(substr($arg, strlen('--file-chunk-max-size=')));
                continue;
            }
            if (strpos($arg, '--large-file-threshold=') === 0) {
                self::$large_file_threshold = self::convert_to_bytes(substr($arg, strlen('--large-file-threshold=')));
                continue;
            }
            if (strpos($arg, '--user-agent=') === 0) {
                self::$user_agent = substr($arg, strlen('--user-agent='));
                continue;
            }
            if (strpos($arg, '--cookie=') === 0) {
                self::$cookie = substr($arg, strlen('--cookie='));
                continue;
            }
            
            // --- 3. Exclusions ---
            if ($arg === '-x') {
                // Check if next arg exists and is not another flag
                if (isset($argv[$i + 1]) && strpos($argv[$i + 1], '-') !== 0) {
                    self::$exclude_paths[] = rtrim($argv[$i + 1], '/');
                    $i++; // Skip the next argument as we consumed it
                } else {
                    echo "Error: Missing value for -x option.\n";
                    exit(1);
                }
                continue;
            }
            if (strpos($arg, '--exclude-tables=') === 0) {
                $val = substr($arg, strlen('--exclude-tables='));
                self::$exclude_tables = explode(',', $val);
                continue;
            }

            // --- 4. Positional Argument (Sync Folder) ---
            // If it's not a flag (start with -) and we haven't found a folder yet
            if (strpos($arg, '-') !== 0 && $folder === null) {
                $folder = $arg;
            } elseif (strpos($arg, '-') !== 0) {
                // Determine if this is an unknown argument or extra positional
                echo "Warning: Ignoring unknown argument '$arg'.\n";
            }
        }

        return $folder;
    }

    public static function main($argv, $argc) {
        if ($argc < 2) {
            self::showHelp();
            exit(0);
        }

        $command = $argv[1];

        // 1. Handle "Standalone" Commands
        // These do not require a site URL or configuration file to run.
        switch ($command) {
            case 'version':
                self::version();
                exit(0);
            case 'upgrade':
                self::upgrade();
                exit(0);
            case 'list':
                self::sites();
                exit(0);
            case 'connect':
                if ($argc != 4) {
                    echo "Error: Invalid arguments for connect\n";
                    exit(1);
                }
                self::connect($argv[2], $argv[3]);
                exit(0);
            case 'remove':
                if ($argc < 3) {
                    echo "Error: Missing <site-url> argument.\n";
                    exit(1);
                }
                self::remove($argv[2]);
                exit(0);
        }

        // 2. Bootstrapping for "Site" Commands (backup, sync, ncdu)
        // These commands require a <site-url> as the second argument.
        if ($argc < 3) {
            echo "Error: Missing <site-url> argument.\n";
            self::showHelp();
            exit(1);
        }

        // A. Set Global Site URL
        self::$siteUrl = $argv[2]; 

        // B. Load Config from disk
        // This attempts to find the token for self::$siteUrl in .disembark
        self::load_config(self::$siteUrl);

        // C. Parse Flags
        $sync_folder = self::parseArguments($argv, 3);

        // 3. Dispatch Command
        switch ($command) {
            case 'backup':
                self::backup();
                break;

            case 'sync':
                // Logic to determine destination folder
                if ($sync_folder === null) {
                    // Default to domain name
                    $sync_folder = preg_replace('/^https?:\/\/(www\.)?/', '', rtrim(self::$siteUrl, '/'));
                }

                // Resolve to absolute path
                if (substr($sync_folder, 0, 1) !== DIRECTORY_SEPARATOR && !(strlen($sync_folder) > 1 && substr($sync_folder, 1, 1) === ':')) {
                    $sync_folder = getcwd() . DIRECTORY_SEPARATOR . $sync_folder;
                }
                
                self::sync($sync_folder);
                break;

            case 'ncdu':
                self::ncdu();
                break;

            case 'info':
                self::info();
                break;

            default:
                echo "Error: Unknown command '$command'.\n";
                self::showHelp();
                exit(1);
        }
    }

    private static function connect($siteUrl, $token) {
        // Trim trailing slashes from the site URL
        $siteUrl = rtrim($siteUrl, '/');
        // Verify the site URL starts with http:// or https://
        if (!preg_match('/^https?:\/\//', $siteUrl)) {
            $siteUrl = "https://$siteUrl";
            echo "The site URL must start with http:// or https://. Attempting to use $siteUrl\n";
        }

        $homeDir = getenv('HOME');
        if (!$homeDir) {
            echo "Error: Unable to determine the home directory.\n";
            exit(1);
        }

        if (empty($siteUrl) || empty($token)) {
            echo "Error: Required arguments <site-url> and <token>.\n";
            exit(1);
        }
        try {
            // Test connection by fetching database info
            $response = \WpOrg\Requests\Requests::get("$siteUrl/wp-json/disembark/v1/database?token=$token", [], ['verify' => false, 'timeout' => 60]);
        } catch (\WpOrg\Requests\Exception $e) {
            // Handle the exception
            echo "Error: Request failed with error: {$e->getMessage()}\n";
            exit(1);
        }

        if ($response->status_code != 200) {
            echo "Error: Failed to connect to $siteUrl. Status code: {$response->status_code}. Please check your URL, token, and ensure the Disembark plugin is active.\n";
            exit(1);
        }

        $tables = json_decode($response->body);
        if (empty($tables)) {
            echo "Error: Connected to $siteUrl, but failed to retrieve database information. Please check plugin functionality.\n";
            exit(1);
        }

        $filePath = $homeDir . '/.disembark';
        $data = [];
        // Check if the file already exists and read its contents
        if (file_exists($filePath)) {
            $jsonContents = file_get_contents($filePath);
            $data = json_decode($jsonContents);
            // Ensure $data is always an array for consistent processing
            if (!is_array($data)) {
                // Try decoding as an object first for legacy compatibility
                if (is_object($data)) {
                    // Convert object to array of objects
                    $data = [$data];
                } else {
                    $data = []; // Initialize as empty array if decoding failed or was invalid
                }
            }
        }

        // Check if the siteUrl already exists and update it, otherwise add new
        $found = false;
        foreach ($data as $key => $entry) {
            // Ensure entry is an object before accessing properties
            if (is_object($entry) && !empty($entry->siteUrl) && $entry->siteUrl === $siteUrl) {
                $data[$key]->token = $token; // Update token directly on the object within the array
                $found = true;
                break;
            }
        }

        if (!$found) {
            // Add new entry as an object
            $data[] = (object) [
                'siteUrl' => $siteUrl,
                'token' => $token
            ];
        }

        $jsonData = json_encode($data, JSON_PRETTY_PRINT);
        if (file_put_contents($filePath, $jsonData) === false) {
            echo "Error: Unable to write to $filePath.\n";
            exit(1);
        }

        echo "Successfully connected to $siteUrl and saved credentials to $filePath\n";
    }

    private static function remove($siteUrl) {
        // Normalize the URL to ensure it matches how it's stored
        $siteUrl = rtrim($siteUrl, '/');
        if (!preg_match('/^https?:\/\//', $siteUrl)) {
            $siteUrl = "https://$siteUrl";
        }

        $homeDir = getenv('HOME');
        if (!$homeDir) {
            echo "Error: Unable to determine the home directory.\n";
            exit(1);
        }

        $filePath = $homeDir . '/.disembark';
        
        if (!file_exists($filePath)) {
            echo "Error: No configuration file found at $filePath.\n";
            exit(1);
        }

        $jsonContents = file_get_contents($filePath);
        $data = json_decode($jsonContents);
        
        // Handle legacy single-object format or invalid data
        if (!is_array($data)) {
            $data = is_object($data) ? [$data] : [];
        }

        $originalCount = count($data);
        $newData = [];
        $found = false;

        foreach ($data as $entry) {
            if (is_object($entry) && isset($entry->siteUrl) && $entry->siteUrl === $siteUrl) {
                $found = true;
                continue; // Skip this entry to remove it
            }
            $newData[] = $entry;
        }

        if ($found) {
            $jsonData = json_encode($newData, JSON_PRETTY_PRINT);
            if (file_put_contents($filePath, $jsonData) === false) {
                echo "Error: Unable to write to $filePath.\n";
                exit(1);
            }
            echo "Successfully removed credentials for $siteUrl\n";
        } else {
            echo "No credentials found for $siteUrl\n";
        }
    }

    private static function make_request($endpoint, $payload = [], $method = 'POST') {
        $headers = ['Content-Type' => 'application/json; charset=utf-8'];
        $data = array_merge(['token' => self::$token, 'backup_token' => self::$backup_token], $payload);
        $url = self::$siteUrl . "/wp-json/disembark/v1" . $endpoint;
        if (self::$cookie) {
            $headers['Cookie'] = self::$cookie;
            
            // Critical: Cloudflare often blocks requests without a valid Referer matching the cookie's domain
            $headers['Referer'] = self::$siteUrl . '/'; 
            
            // Critical: Missing these standard headers flags the request as a bot
            $headers['Accept-Language'] = 'en-US,en;q=0.9';
            $headers['Accept'] = 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8';
        }
        $options = [
            'verify'    => false,
            'timeout'   => 600,
            'useragent' => self::$user_agent
        ];

        try {
            switch (strtoupper($method)) {
                case 'POST':
                    // Increase timeout specifically for zip operations which can take longer
                    if ($endpoint === '/zip-database' || $endpoint === '/zip-sync-files') {
                        $options['timeout'] = 1800; // 30 minutes
                    } elseif (strpos($endpoint, '/export/database/') === 0 && isset($payload['parts'])) {
                        $options['timeout'] = 1200; // 20 minutes for large table parts
                    }
                    $response = \WpOrg\Requests\Requests::post($url, $headers, json_encode($data), $options);
                    break;
                case 'GET':
                default:
                    // Increase timeout for cleanup as well
                    if ($endpoint === '/cleanup') {
                        $options['timeout'] = 120;
                    }
                    // For GET, build query params correctly
                    $query_params = http_build_query($data);
                    $response = \WpOrg\Requests\Requests::get($url . '?' . $query_params, $headers, $options);
                    break;
            }

            if ($response->status_code !== 200) {
                $errorMessage = "Error: Request to {$endpoint} failed. HTTP status: {$response->status_code}.";
                $errorBody = json_decode($response->body);
                if (json_last_error() === JSON_ERROR_NONE && isset($errorBody->message)) {
                    $errorMessage .= " Message: " . $errorBody->message;
                } else {
                    $errorMessage .= " Response: " . substr($response->body, 0, 500);
                }
                echo $errorMessage . "\n";
                // Throw an exception instead of exiting directly to allow potential cleanup
                throw new \Exception("API request failed for {$endpoint}");
            }

            $decoded = json_decode($response->body);
            // Check if this endpoint is allowed to return a raw string (like a path or URL)
            $allowed_raw_endpoints = ['/cleanup', '/zip-database', '/zip-sync-files'];
            $is_raw_endpoint = false;
            foreach ($allowed_raw_endpoints as $raw_ep) {
                if ($endpoint === $raw_ep) $is_raw_endpoint = true;
            }
            if (strpos($endpoint, '/export/database/') === 0) {
                $is_raw_endpoint = true;
            }

            if (json_last_error() !== JSON_ERROR_NONE) {
                // Not JSON.
                // Is it an endpoint that *should* return a raw string?
                if (!empty(trim($response->body)) && $is_raw_endpoint) {
                    return trim($response->body); // Return the raw body (e.g., URL)
                }
                // Not JSON and not an allowed raw endpoint, or it's empty
                if (!empty(trim($response->body))) {
                    echo "Error: Could not decode JSON response from {$endpoint}. Response: " . substr($response->body, 0, 500) . "\n";
                    throw new \Exception("Invalid JSON response for {$endpoint}");
                }
                // Allow empty responses for cleanup
                if ($endpoint === '/cleanup') {
                    return "Cleanup requested";
                }
            }

            // If it *is* valid JSON, but the API might have returned a JSON-encoded string (e.g. "path\/to\/file.sql")
            // json_decode will have correctly turned this into a PHP string.
            // If it was a JSON object/array, it will be a PHP object/array.
            return $decoded;
        } catch (\WpOrg\Requests\Exception $e) {
            echo "Error: Request failed for {$endpoint}: {$e->getMessage()}\n";
            // Re-throw or handle as needed
            throw $e; // Re-throw to allow potential cleanup in calling function
        }
    }

    /**
     * Runs the remote manifest generation process.
     * Can optionally request file checksums for sync operations.
     * @return array List of manifest chunk file objects.
     */
    private static function runManifestGeneration($include_checksums = false) {
        $files_manifests = [];
        try {
            echo "Running analysis (this may take a while)...\n";
            $exclude_files_string = implode("\n", self::$exclude_paths);

            $payload = [
                'exclude_files' => $exclude_files_string
            ];
            if ($include_checksums) {
                $payload['include_checksums'] = true;
            }

            // Step 1: Initiate
            // Simplified make_request signature: (endpoint, payload, method)
            self::make_request('/regenerate-manifest', ['step' => 'initiate'] + $payload, 'POST');
            echo "Initiated file scan.\n";

            // Step 2: Scan Loop
            $scan_complete = false;
            $last_scan_output = "";
            while (!$scan_complete) {
                $scan_response = self::make_request('/regenerate-manifest', ['step' => 'scan'] + $payload, 'POST');
                
                if (!is_object($scan_response) || !isset($scan_response->status)) {
                    throw new \Exception("Invalid response received during scan step.");
                }

                if ($scan_response->status === 'scan_complete') {
                    $scan_complete = true;
                    echo str_pad("", strlen($last_scan_output), " ") . "\r";
                    printf("Scanning complete. Analyzed %d directories.\n", $scan_response->total_dirs ?? 0);
                } else if (isset($scan_response->scanned_dirs) && isset($scan_response->total_dirs)) {
                    $last_scan_output = sprintf("Scanning... (%d / %d directories)", $scan_response->scanned_dirs, $scan_response->total_dirs);
                    echo $last_scan_output . "\r";
                }
                usleep(250000);
            }

            // Step 3: Chunkify
            echo "Chunkifying file list...\n";
            $chunk_response = self::make_request('/regenerate-manifest', ['step' => 'chunkify'] + $payload, 'POST');
            $total_chunks = $chunk_response->total_chunks ?? 0;
            
            if ($total_chunks === 0) {
                echo "No files found to analyze or an error occurred during chunkification.\n";
            } else {
                echo "File list divided into {$total_chunks} chunks.\n";
            }

            // Step 4: Process Chunks
            $last_chunk_output = "";
            for ($i = 1; $i <= $total_chunks; $i++) {
                $last_chunk_output = sprintf("Processing chunk %d of %d...", $i, $total_chunks);
                echo $last_chunk_output . "\r";
                self::make_request('/regenerate-manifest', ['step' => 'process_chunk', 'chunk' => $i] + $payload, 'POST');
                usleep(50000);
            }
            if ($total_chunks > 0) {
                echo str_pad("", strlen($last_chunk_output), " ") . "\r";
                echo "Chunk processing complete.\n";
            }

            // Step 5: Finalize
            $files_manifests = self::make_request('/regenerate-manifest', ['step' => 'finalize'] + $payload, 'POST');
            
            if (!is_array($files_manifests)) {
                echo "Warning: Final manifest data is not in the expected format. File stats may be inaccurate.\n";
                $files_manifests = [];
            }
            echo "Analysis complete.\n";

        } catch (\Exception $e) {
            echo "Error during file analysis: {$e->getMessage()}\n";
            self::cleanupTemporaryFiles();
            exit(1);
        }

        return $files_manifests;
    }

    private static function displayBackupSummary($files_manifests) {
        // 1. Fetch database info (using simplified make_request)
        $all_tables = [];
        try {
            $all_tables = self::make_request('/database', [], 'GET');
            if (!is_array($all_tables)) $all_tables = [];
        } catch (\Exception $e) {
            echo "Error: Failed to fetch database info: {$e->getMessage()}\n";
            exit(1);
        }

        // 2. Calculate file stats
        $remainingFiles = empty($files_manifests) ? 0 : array_sum(array_column($files_manifests, 'count'));
        $remainingSize = empty($files_manifests) ? 0 : array_sum(array_column($files_manifests, 'size'));

        // 3. Calculate database stats
        $totalTables = count($all_tables);
        $totalDbSize = empty($all_tables) ? 0 : array_sum(array_column($all_tables, 'size'));
        $excludedTables = 0;
        $excludedDbSize = 0;
        $included_table_list = [];

        foreach ($all_tables as $table) {
            $table_size = isset($table->size) && is_numeric($table->size) ? (float)$table->size : 0;
            $is_excluded = false;
            
            // Check against self::$exclude_tables
            foreach (self::$exclude_tables as $pattern) {
                if (fnmatch($pattern, $table->table)) {
                    $is_excluded = true;
                    break;
                }
            }

            if ($is_excluded) {
                $excludedTables++;
                $excludedDbSize += $table_size;
            } else {
                $table->size = $table_size;
                $included_table_list[] = $table;
            }
        }
        $remainingTables = $totalTables - $excludedTables;
        $remainingDbSize = empty($included_table_list) ? 0 : array_sum(array_column($included_table_list, 'size'));

        // 4. Display results
        echo "\nBackup Preview: " . self::$siteUrl . "\n";
        echo "* Exclusions are applied server-side during analysis.\n";
        echo "* The summary below shows only files and tables scheduled for backup.\n\n";
        
        echo "File Backup Summary\n";
        printf("%-24s %12s %12s\n", "", "Count", "Size");
        printf("%-24s %12s %12s\n", str_repeat('-', 24), str_repeat('-', 12), str_repeat('-', 12));
        printf("%-24s %12s %12s\n", "Files to be Backed Up:", number_format($remainingFiles), self::humanFilesize($remainingSize));

        echo "\nDatabase Backup Summary\n";
        printf("%-24s %12s %12s\n", "", "Count", "Size");
        printf("%-24s %12s %12s\n", str_repeat('-', 24), str_repeat('-', 12), str_repeat('-', 12));
        printf("%-24s %12s %12s\n", "Total Tables:", number_format($totalTables), self::humanFilesize($totalDbSize));
        printf("%-24s %12s %12s\n", "Excluded Tables:", number_format($excludedTables), self::humanFilesize($excludedDbSize));
        printf("%-24s %12s %12s\n", "Tables to be Backed Up:", number_format($remainingTables), self::humanFilesize($remainingDbSize));
        echo "\n* This is an estimate. Actual backup size may vary.\n";
    }

    private static function runPreview($is_preview_mode) {

        // 1. Run the manifest generation
        $files_manifests = self::runManifestGeneration(false);

        // 2. Display the summary
        self::displayBackupSummary($files_manifests);

        // 3. Handle Preview Exit
        if ($is_preview_mode) {
            self::cleanupTemporaryFiles();
            exit(0);
        }

        // 4. Return the generated manifest for the actual backup
        return $files_manifests;
    }

    /**
     * Renders a CLI progress bar string.
     */
    private static function draw_progress_bar($done, $total, $width = 20) {
        if ($total < 1) return str_repeat(' ', $width + 7);
        $perc = min(100, round(($done / $total) * 100));
        $bar_filled = round(($width * $perc) / 100);
        $bar = '[' . str_repeat('=', $bar_filled) . str_repeat(' ', $width - $bar_filled) . ']';
        return sprintf("%s %3d%%", $bar, $perc);
    }

    /**
     * Helper function specifically for cleanup after preview or backup failure
     */
    private static function cleanupTemporaryFiles() {
        echo "Cleaning up temporary files on server...\n";
        try {
            $payload = ['backup_token' => self::$backup_token];
            self::make_request('/cleanup', $payload, 'GET');
        } catch (\Exception $e) {
            echo "Warning: Cleanup request failed: {$e->getMessage()}\n";
        }
    }

    /**
     * Helper to download large files via public URL.
     */
    private static function fetch_large_file_publicly($file, $local_destination) {
        $file_size_human = self::humanFilesize($file->size);
        echo "   > Large file detected ($file_size_human). Attempting public direct download...\n";
        
        $path_parts = explode('/', $file->name);
        $encoded_parts = array_map('rawurlencode', $path_parts);
        $public_url = self::$siteUrl . '/' . implode('/', $encoded_parts);

        echo "     Downloading: $public_url\n";
        try {
            self::download_file_direct($public_url, $local_destination);
            echo "     - Download complete.\n";
            return true;
        } catch (\Exception $e) {
            echo "     - Public download failed: {$e->getMessage()}. Falling back to stream...\n";
            return false;
        }
    }

    /**
     * Downloads a large file via the stream API in chunks (e.g., 50MB) to avoid PHP timeouts.
     */
    private static function download_file_chunked_stream($relative_path, $destination, $total_size) {
        $chunk_size = 50 * 1024 * 1024; // 50 MB chunks
        $offset = 0;
        
        // Open destination for writing
        $fp = fopen($destination, 'w');
        if (!$fp) throw new \Exception("Could not open local file for writing: $destination");
        fclose($fp);

        $progress_bar_width = 20;
        while ($offset < $total_size) {
            $length = min($chunk_size, $total_size - $offset);
            
            // Calculate progress
            $percent = min(100, round(($offset / $total_size) * 100));
            $downloaded_human = self::humanFilesize($offset);
            $total_human = self::humanFilesize($total_size);
            
            echo str_pad("", 80, " ") . "\r"; // Clear line
            echo "     - Streaming Chunk: {$downloaded_human} / {$total_human} ({$percent}%)...\r";
            
            $attempts = 0;
            $max_attempts = 5;
            $chunk_success = false;

            while ($attempts < $max_attempts && !$chunk_success) {
                $attempts++;
                try {
                    // UPDATED: Use self::$siteUrl
                    $endpoint_url = self::$siteUrl . '/wp-json/disembark/v1/stream-file';
                    
                    // UPDATED: Use self::$token
                    $payload = json_encode([
                        'token' => self::$token, 
                        'file' => $relative_path,
                        'offset' => $offset,
                        'length' => $length
                    ]);

                    $headers = ['Content-Type' => 'application/json'];
                    
                    $options = [
                        'verify' => false,
                        'timeout' => 300, // 5 minutes per 50MB chunk should be plenty
                        'connect_timeout' => 30,
                    ];
                    
                    $response = \WpOrg\Requests\Requests::post($endpoint_url, $headers, $payload, $options);

                    if (!$response->success) {
                         throw new \Exception("HTTP {$response->status_code}");
                    }
                    
                    if (strlen($response->body) != $length) {
                        // Sometimes server might send less if EOF, but we calculated EOF.
                    }

                    // Append to file
                    file_put_contents($destination, $response->body, FILE_APPEND);
                    $chunk_success = true;

                } catch (\Exception $e) {
                    if ($attempts < $max_attempts) {
                        echo str_pad("", 80, " ") . "\r"; 
                        echo "     - Retry ($attempts/$max_attempts) for offset $offset...\r";
                        sleep(2);
                    } else {
                        throw new \Exception("Failed to download chunk at offset $offset: " . $e->getMessage());
                    }
                }
            }
            
            $offset += $length;
        }
        
        echo str_pad("", 80, " ") . "\r";
        $total_human = self::humanFilesize($total_size);
        echo "     - Streaming complete: {$total_human} downloaded.\n";
    }

    /**
     * Processes the database backup using hybrid batching.
     *
     * @param array $tables_to_backup List of table objects to be backed up.
     * @param string $db_export_file The final, local SQL file path to append to.
     * @param string $local_db_parts_path A temporary directory to store downloaded SQL chunks.
     * @throws \Exception
     */
    private static function processDatabaseBackup($tables_to_backup, $db_export_file, $local_db_parts_path) {
        
        echo "Backing up database tables...\n";
        
        // Use static thresholds
        $max_size = self::$db_max_size;
        $max_rows = self::$db_max_rows;

        // --- Hybrid Batching Logic ---
        $large_tables = [];
        $small_tables = [];
        foreach ($tables_to_backup as $table) {
            $table_size = isset($table->size) && is_numeric($table->size) ? (float)$table->size : 0;
            $row_count = isset($table->row_count) && is_numeric($table->row_count) ? (int)$table->row_count : 0;
            if (($table_size > $max_size || $row_count > $max_rows) && $row_count > 0) {
                $parts_by_size = ceil($table_size / $max_size);
                $parts_by_rows = ceil($row_count / $max_rows);
                $parts = max($parts_by_size, $parts_by_rows);
                
                $table->parts = $parts;
                $table->current = 0;
                $table->rows_per_part = ceil($row_count / $parts);
                $large_tables[] = $table;
            } else {
                $table->parts = 0;
                $small_tables[] = $table;
            }
        }

        $small_table_batches = [];
        $current_batch = [];
        $current_batch_size = 0;
        foreach ($small_tables as $table) {
            $table_size = isset($table->size) && is_numeric($table->size) ? (float)$table->size : 0;
            if ($current_batch_size + $table_size > $max_size && !empty($current_batch)) {
                $small_table_batches[] = $current_batch;
                $current_batch = [];
                $current_batch_size = 0;
            }
            $current_batch[] = $table;
            $current_batch_size += $table_size;
        }
        if (!empty($current_batch)) {
            $small_table_batches[] = $current_batch;
        }
        // --- End Hybrid Batching ---

        // Calculate total steps
        $total_db_steps = count($small_table_batches);
        foreach ($large_tables as $table) {
            $total_db_steps += ($table->parts > 0) ? $table->parts : 1;
        }
        $current_db_step = 0;
        
        // --- Loop 1: Process small table batches ---
        foreach ($small_table_batches as $batch) {
            $current_db_step++;
            $table_names = array_column($batch, 'table');
            $batch_size = array_sum(array_column($batch, 'size'));
            $progress_message = sprintf(" - Exporting batch %d/%d (%d tables, %s)", $current_db_step, $total_db_steps, count($batch), self::humanFilesize($batch_size));
            echo $progress_message . "...\r";

            // Simplified make_request
            $file_url = self::make_request("/export-database-batch", ['tables' => $table_names], 'POST');
            
            if (strpos($file_url, 'http') !== 0) {
                throw new \Exception("Export failed for batch, did not receive a valid URL. Got: $file_url");
            }
            $file_name = basename($file_url);
            $local_file_path = $local_db_parts_path . '/' . $file_name;

            $parsed_url = parse_url($file_url);
            $relative_path = ltrim($parsed_url['path'], '/');
            
            // Simplified download call
            self::download_db_file($relative_path, $local_file_path);

            file_put_contents($db_export_file, file_get_contents($local_file_path), FILE_APPEND);
            unlink($local_file_path); 

            self::make_request('/cleanup-file', ['file_name' => $file_name], 'POST');
            echo $progress_message . " - Done.\n";
            usleep(50000);
        }

        // --- Loop 2: Process large tables ---
        foreach ($large_tables as $table) {
            if (!isset($table->table)) {
                echo "\nWarning: Skipping invalid table data entry in backup.\n";
                continue;
            }

            for ($part = 1; $part <= $table->parts; $part++) {
                $current_db_step++;
                $total_table_size = (float)$table->size;
                $row_count = isset($table->row_count) && is_numeric($table->row_count) ? (int)$table->row_count : 0;
                
                $current_part_size = 0;
                $parts_by_size = ($max_size > 0) ? ceil($total_table_size / $max_size) : 1;
                $parts_by_rows = ($max_rows > 0) ? ceil($row_count / $max_rows) : 1;
                
                if ($parts_by_rows > $parts_by_size) {
                    $current_part_size = $total_table_size / $table->parts;
                } else {
                    $current_part_size = $max_size;
                }

                if ($part == $table->parts) {
                    $previous_parts_size = $current_part_size * ($table->parts - 1);
                    $current_part_size = $total_table_size - $previous_parts_size;
                }
                
                $description = sprintf("%s %d/%d", $table->table, $part, $table->parts);
                $size_string = self::humanFilesize(max(0, $current_part_size));
                $progress_message = sprintf(" - Exporting batch %d/%d (%s, %s)", $current_db_step, $total_db_steps, $description, $size_string);
                echo $progress_message . "...\r";
                
                $part_data = ['parts' => $part, 'rows_per_part' => $table->rows_per_part];
                $file_url = self::make_request("/export/database/{$table->table}", $part_data, 'POST');
                
                if (strpos($file_url, 'http') !== 0) {
                    throw new \Exception("Export failed, did not receive a valid URL. Got: $file_url");
                }
                $file_name = basename($file_url);
                $local_file_path = $local_db_parts_path . '/' . $file_name;

                $parsed_url = parse_url($file_url);
                $relative_path = ltrim($parsed_url['path'], '/');
                
                self::download_db_file($relative_path, $local_file_path);
                
                file_put_contents($db_export_file, file_get_contents($local_file_path), FILE_APPEND);
                unlink($local_file_path);

                self::make_request('/cleanup-file', ['file_name' => $file_name], 'POST');
                echo $progress_message . " - Done.\n";
                usleep(50000);
            }
        }
        echo "Database export and download complete.\n";
    }

   /**
     * Synchronizes the remote site to a local folder (Remote -> Local).
     */
    private static function sync($folder) {

        // 1. Setup Session / Backup Token
        if (self::$session_id) {
            self::$backup_token = self::$session_id;
            echo "Reusing backup session: " . self::$backup_token . "\n";
            if (!self::$skip_checksums) {
                self::$skip_checksums = true;
                echo " - Forcing --skip-checksums (GUI sessions do not contain checksum data).\n";
            }
        } else {
            self::$backup_token = substr(bin2hex(random_bytes(20)), 0, -28);
        }

        // 2. Folder Setup
        $is_initial_sync = !is_dir($folder);
        if ($is_initial_sync) {
            echo "Performing initial sync to new folder: $folder\n";
            if (!mkdir($folder, 0755, true)) {
                echo "Error: Could not create folder at $folder.\n";
                exit(1);
            }
        } else {
            echo "Performing subsequent sync to existing folder: $folder\n";
        }

        $local_db_parts_path = $folder . '/.disembark-db-parts';
        $db_export_file = $folder . '/database.sql';
        
        // Initialize Stats
        $transferred_bytes = 0;
        $transferred_files = 0;

        try {
            // --- Database Sync ---
            if (!self::$skip_db) {
                echo "Starting database sync...\n";
                if (!is_dir($local_db_parts_path) && !mkdir($local_db_parts_path, 0755, true)) {
                    throw new \Exception("Could not create temp DB directory.");
                }
                
                if (file_put_contents($db_export_file, "/*!40101 SET NAMES utf8mb4 */;\nSET sql_mode='NO_AUTO_VALUE_ON_ZERO';\n") === false) {
                    throw new \Exception("Could not write to local SQL file.");
                }

                $database = self::make_request('/database', [], 'GET');
                if (!is_array($database)) throw new \Exception("Invalid database list received.");

                // Tally Database Stats
                foreach ($database as $tbl) {
                    $transferred_bytes += (isset($tbl->size) ? (float)$tbl->size : 0);
                }

                self::processDatabaseBackup($database, $db_export_file, $local_db_parts_path);
                self::delete_directory($local_db_parts_path);
            } else {
                echo "Skipping database sync as requested.\n";
            }

            // --- File Sync ---
            if (!self::$skip_files) {
                $files_manifest_chunks = [];
                if (self::$session_id) {
                    echo "Fetching existing manifest for sync...\n";
                    try {
                        $files_manifest_chunks = self::make_request('/manifest', [], 'GET');
                        if (!is_array($files_manifest_chunks)) {
                            throw new \Exception("Could not fetch or parse existing manifest.");
                        }
                        echo "Successfully fetched manifest with " . count($files_manifest_chunks) . " file chunks.\n";
                    } catch (\Exception $e) {
                        echo "Error fetching manifest: {$e->getMessage()}\n";
                        if (!self::$session_id) self::cleanupTemporaryFiles();
                        exit(1);
                    }
                } else {
                    echo "Starting file sync analysis...\n";
                    $include_checksums = (!$is_initial_sync && !self::$skip_checksums);
                    $files_manifest_chunks = self::runManifestGeneration($include_checksums);
                }

                // Initial Sync Logic
                if ($is_initial_sync) {
                    echo "Performing initial file download (Optimized)...\n";
                    echo "Fetching remote file manifest...\n";
                    
                    list($remote_files_map, $all_files_debug) = self::fetch_manifests_parallel($files_manifest_chunks);
                    
                    // Calculate Total Size for Global Progress
                    $total_bytes_to_sync = 0;
                    foreach ($remote_files_map as $f) {
                        $total_bytes_to_sync += (isset($f->size) ? (float)$f->size : 0);
                    }
                    self::$global_bytes_total = $total_bytes_to_sync;
                    self::$global_bytes_processed = 0;
                    
                    // Add to Transferred Stats
                    $transferred_bytes += $total_bytes_to_sync;
                    $transferred_files += count($remote_files_map);

                    $large_files_queue = [];
                    $small_files_queue = [];

                    foreach ($remote_files_map as $file) {
                        $fsize = isset($file->size) ? (int)$file->size : 0;
                        if ($fsize > self::$large_file_threshold) {
                            $large_files_queue[] = $file;
                        } else {
                            $small_files_queue[] = $file;
                        }
                    }

                    if (!empty($large_files_queue)) {
                        self::download_files_parallel($large_files_queue, $folder);
                    }

                    if (!empty($small_files_queue)) {
                        $total_small = count($small_files_queue);
                        
                        $file_chunks = [];
                        $current_chunk = [];
                        $current_chunk_size = 0;
                        foreach ($small_files_queue as $file) {
                            $fsize = $file->size ?? 0;
                            if (($current_chunk_size + $fsize > self::$file_chunk_max_size) || (count($current_chunk) >= self::$file_chunk_size)) {
                                $file_chunks[] = $current_chunk;
                                $current_chunk = [$file];
                                $current_chunk_size = $fsize;
                            } else {
                                $current_chunk[] = $file;
                                $current_chunk_size += $fsize;
                            }
                        }
                        if (!empty($current_chunk)) $file_chunks[] = $current_chunk;
                        
                        // Use the new Parallel State Machine
                        if (!empty($file_chunks)) {
                            self::sync_chunks_parallel($file_chunks, $folder);
                        }
                    }
                    echo "Initial file sync complete.\n";
                } else {
                    // Subsequent Sync (Diff)
                    echo "Fetching remote file manifest...\n";
                    list($remote_files_map, $all_remote_files) = self::fetch_manifests_parallel($files_manifest_chunks);
                    echo str_pad("", 80, " ") . "\r";

                    echo "Generating local file manifest...\n";
                    $local_files_map = self::generate_local_manifest($folder, self::$skip_checksums);
                    echo "Comparing manifests and syncing changes...\n";
                    
                    $files_to_download = [];
                    $files_to_delete = [];
                    foreach ($remote_files_map as $path => $remote_file) {
                        $local_file = $local_files_map[$path] ?? null;
                        $should_download = false;
                        if (self::$skip_checksums) {
                            if (!$local_file || (isset($local_file->size) && isset($remote_file->size) && $local_file->size !== $remote_file->size)) {
                                $should_download = true;
                            }
                        } else {
                            if (!$local_file || !isset($remote_file->checksum) || $local_file->checksum !== $remote_file->checksum) {
                                $should_download = true;
                            }
                        }
                        if ($should_download) $files_to_download[] = $remote_file;
                    }
                    foreach ($local_files_map as $path => $local_file) {
                        if (!isset($remote_files_map[$path])) $files_to_delete[] = $path;
                    }

                    // Stats
                    $total_bytes_to_sync = 0;
                    foreach ($files_to_download as $f) {
                        $total_bytes_to_sync += (isset($f->size) ? (float)$f->size : 0);
                    }
                    self::$global_bytes_total = $total_bytes_to_sync;
                    self::$global_bytes_processed = 0;

                    $transferred_bytes += $total_bytes_to_sync;
                    $transferred_files += count($files_to_download);

                    if (!empty($files_to_download)) {
                        $total_files_to_download = count($files_to_download);
                        echo " - Syncing $total_files_to_download new/changed file(s)...\n";
                        
                        $file_chunks = [];
                        $current_chunk = [];
                        $current_chunk_size = 0;
                        $large_files_parallel_queue = [];
                        foreach ($files_to_download as $file) {
                            $fsize = $file->size ?? 0;
                            
                            if ($fsize > self::$large_file_threshold) {
                                if (!empty($current_chunk)) { 
                                    $file_chunks[] = $current_chunk;
                                    $current_chunk = []; 
                                    $current_chunk_size = 0; 
                                }
                                $large_files_parallel_queue[] = $file;
                                continue;
                            }
                            
                            if (($current_chunk_size + $fsize > self::$file_chunk_max_size) || (count($current_chunk) >= self::$file_chunk_size)) {
                                $file_chunks[] = $current_chunk;
                                $current_chunk = [$file];
                                $current_chunk_size = $fsize;
                            } else {
                                $current_chunk[] = $file;
                                $current_chunk_size += $fsize;
                            }
                        }
                        if (!empty($current_chunk)) $file_chunks[] = $current_chunk;

                        if (!empty($large_files_parallel_queue)) {
                            self::download_files_parallel($large_files_parallel_queue, $folder);
                        }

                        // Use the new Parallel State Machine
                        if (!empty($file_chunks)) {
                            self::sync_chunks_parallel($file_chunks, $folder);
                        }

                        echo " - All sync chunks processed.\n";
                    } else {
                        echo " - No new or changed files to download.\n";
                    }

                    if (!empty($files_to_delete)) {
                        foreach ($files_to_delete as $path) {
                            echo " - Removing: $path\n";
                            $f = $folder . DIRECTORY_SEPARATOR . $path;
                            if (file_exists($f)) unlink($f);
                        }
                    }

                    echo "Cleaning up empty directories...\n";
                    self::delete_empty_dirs($folder);
                    echo "File sync complete.\n";
                }
            } else {
                echo "Skipping file sync as requested.\n";
            }
        } catch (\Exception $e) {
            echo "\033[?25h"; 
            echo "\nSync failed: {$e->getMessage()}\n";
            if (!self::$session_id) {
                self::cleanupTemporaryFiles();
            }
            exit(1);
        }

        if (!self::$session_id) {
            self::cleanupTemporaryFiles();
        } else {
            echo "\nSkipping remote cleanup to preserve UI session files.\n";
        }
        
        // Wait for background extractions
        $wait_ticks = 0;
        // The background logic in sync_chunks_parallel creates zips.
        // We verify that they are gone before finishing.
        while (count(glob($folder . '/sync-files-*.zip')) > 0 || count(glob($folder . '/files-*.zip')) > 0) {
            if ($wait_ticks === 0) echo "Waiting for background extraction to finish...";
            echo ".";
            sleep(1);
            $wait_ticks++;
            if ($wait_ticks > 60) {
                 echo "\nWarning: Background extraction taking too long. Some zip files may remain in $folder.\n";
                 break;
            }
        }
        if ($wait_ticks > 0) echo " Done.\n";

        // Final Output with Stats
        $stats_str = self::humanFilesize($transferred_bytes);
        $files_str = number_format($transferred_files);
        echo "\nSync Completed Successfully\n";
        echo "-----------------------------\n";
        printf("%-15s %s\n", "Destination:", $folder);
        printf("%-15s %s\n", "Transferred:", self::humanFilesize($transferred_bytes));
        printf("%-15s %s\n", "Items:", number_format($transferred_files));
        echo "\n";
    }

    /**
     * Processes zip chunks in parallel using a state machine (Zip -> Download -> Background Unzip).
     * Concurrency: 2 (Keeps pressure on server without overloading).
     */
    private static function sync_chunks_parallel($chunks, $folder) {
        // Increase concurrency since unzip is now non-blocking
        $max_concurrency = 2;
        $max_retries = 3;
        $mh = curl_multi_init();
        
        // 1. Prepare Queue
        $queue = [];
        $chunk_values = array_values($chunks);
        foreach ($chunk_values as $i => $files) {
            $queue[] = [
                'files' => $files,
                'attempts' => 0,
                'id' => $i + 1 // Stable ID for UI
            ];
        }
        
        $total_chunks = count($queue);
        $fallback_chunks = [];
        $active_transfers = [];
        $slots = array_fill(0, $max_concurrency, null); 

        echo "\033[?25l";
        echo " - Starting parallel sync (Concurrency: $max_concurrency, Retries: $max_retries)...\n";
        echo str_repeat("\n", $max_concurrency);
        echo "\033[{$max_concurrency}A";

        do {
            // 2. REFILL SLOTS
            while (count($active_transfers) < $max_concurrency && !empty($queue)) {
                $slot_idx = array_search(null, $slots);
                if ($slot_idx === false) $slot_idx = count($active_transfers);
                
                // Get Job
                $job = array_shift($queue);
                $job['attempts']++;
                
                // Start STAGE_ZIP
                $url = self::$siteUrl . "/wp-json/disembark/v1/zip-sync-files";
                $payload = json_encode([
                    'token' => self::$token, 
                    'backup_token' => self::$backup_token, 
                    'files' => $job['files']
                ]);

                $ch = curl_init($url);
                
                if ($ch === false) {
                    $fallback_chunks[] = $job['files'];
                    continue;
                }

                curl_setopt($ch, CURLOPT_POST, 1);
                curl_setopt($ch, CURLOPT_POSTFIELDS, $payload);
                curl_setopt($ch, CURLOPT_HTTPHEADER, ['Content-Type: application/json']);
                if (self::$cookie) {
                    curl_setopt($ch, CURLOPT_COOKIE, self::$cookie);
                }
                curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
                curl_setopt($ch, CURLOPT_SSL_VERIFYPEER, false);
                curl_setopt($ch, CURLOPT_TIMEOUT, 1800);
                curl_setopt($ch, CURLOPT_USERAGENT, self::$user_agent);
                curl_multi_add_handle($mh, $ch);
                $ch_id = is_object($ch) ? spl_object_id($ch) : (int)$ch;
                
                $active_transfers[$ch_id] = [
                    'state' => 'ZIP',
                    'chunk_idx' => $job['id'],
                    'slot' => $slot_idx,
                    'job' => $job, 
                    'file_count' => count($job['files'])
                ];
                $slots[$slot_idx] = $ch_id;
            }

            // 3. EXECUTE
            curl_multi_exec($mh, $running);
            if ($running > 0 && curl_multi_select($mh, 0.1) === -1) {
                usleep(100);
            }

            // 4. UPDATE UI
            foreach ($active_transfers as $data) {
                $slot = $data['slot'];
                $line = $slot + 1;
                $idx = $data['chunk_idx'];
                $count = $data['file_count'];
                $attempt = $data['job']['attempts'];
                $retry_label = ($attempt > 1) ? "[Retry] " : "";
                $label_num = $slot + 1;

                // --- UI ALIGNMENT LOGIC ---
                $status_text = "";
                $bar_output = "";
                $size_output = "";

                if ($data['state'] === 'ZIP') {
                    $status_text = "Chunk $idx/$total_chunks ($count files): {$retry_label}Requesting Zip...";
                } elseif ($data['state'] === 'DL') {
                    $d = curl_getinfo($data['handle'], CURLINFO_SIZE_DOWNLOAD);
                    $t = curl_getinfo($data['handle'], CURLINFO_CONTENT_LENGTH_DOWNLOAD);
                    
                    $status_text = "Chunk $idx/$total_chunks: {$retry_label}Downloading...";
                    $bar_output = ($t > 0) ? self::draw_progress_bar($d, $t, 15) : '';
                    $size_output = self::humanFilesize($d);
                } elseif ($data['state'] === 'UNZIP') {
                    $status_text = "Chunk $idx/$total_chunks: Background Extracting...";
                }

                if (strlen($status_text) > 56) $status_text = '...' . substr($status_text, -53);
                $status_padded = str_pad($status_text, 56);
                
                echo "\033[{$line}B\r\033[K {$label_num}: {$status_padded} {$bar_output} {$size_output}\033[{$line}A";
            }

            // 5. PROCESS SYNCHRONOUS TASKS (UNZIP)
            foreach ($active_transfers as $id => $data) {
                if ($data['state'] === 'UNZIP') {
                    
                    $slot = $data['slot'];
                    $line = $slot + 1;
                    $label_num = $slot + 1;

                    try {
                        // --- NON-BLOCKING BACKGROUND EXTRACTION ---
                        // We use php -r to spawn a detached process that handles ZipArchive and unlink.
                        $zip_path = $data['file_path'];
                        $dest_path = $folder;

                        // Escape for shell safety
                        $safe_zip = escapeshellarg($zip_path);
                        $safe_dest = escapeshellarg($dest_path);

                        // PHP One-Liner: Open, Extract, Close, Delete.
                        $php_code = sprintf(
                            '$z = new ZipArchive; if ($z->open(%s) === TRUE) { $z->extractTo(%s); $z->close(); unlink(%s); }',
                            $safe_zip,
                            $safe_dest,
                            $safe_zip
                        );
                        
                        // Execute in background (> /dev/null 2>&1 &)
                        $cmd = sprintf('php -r %s > /dev/null 2>&1 &', escapeshellarg($php_code));
                        exec($cmd);

                        // Update global stats immediately (based on zip size)
                        // Note: Uncompressed size would be more accurate but requires reading the zip, which blocks.
                        self::$global_bytes_processed += filesize($zip_path);

                        $msg_text = "Chunk {$data['chunk_idx']} backgrounded.";
                        $msg_padded = str_pad($msg_text, 56);
                        echo "\033[{$line}B\r\033[K {$label_num}: {$msg_padded}\033[{$line}A";

                        // Cleanup remote file immediately
                        try {
                            self::make_request('/cleanup-file', ['file_name' => $data['file_name']], 'POST');
                        } catch (\Exception $e) { }
                        
                    } catch (\Exception $e) {
                         // Should rarely happen with exec, but handle logic errors
                         $fallback_chunks[] = $data['job']['files'];
                         echo "\033[{$line}B\r\033[K {$label_num}: Background Launch Failed.\033[{$line}A";
                    }

                    // Done with this job immediately - DO NOT WAIT for unzip
                    $slots[$slot] = null;
                    unset($active_transfers[$id]);
                }
            }

            // 6. CHECK ASYNC COMPLETION (cURL)
            while ($info = curl_multi_info_read($mh)) {
                $ch = $info['handle'];
                $ch_id = is_object($ch) ? spl_object_id($ch) : (int)$ch;
                
                if (!isset($active_transfers[$ch_id])) continue;
                
                $data = $active_transfers[$ch_id];
                $slot = $data['slot'];
                $line = $slot + 1;
                $label_num = $slot + 1;
                
                curl_multi_remove_handle($mh, $ch);
                if ($data['state'] === 'ZIP') {
                    // ZIP FINISHED logic...
                    $raw_response = curl_multi_getcontent($ch);
                    $http_code = curl_getinfo($ch, CURLINFO_HTTP_CODE);
                    
                    if (is_resource($ch)) curl_close($ch);
                    
                    $zip_url = null;
                    $decoded = json_decode($raw_response);
                    if (json_last_error() === JSON_ERROR_NONE && is_string($decoded)) {
                        $zip_url = $decoded;
                    } elseif (strpos($raw_response, 'http') === 0) {
                        $zip_url = trim($raw_response);
                    }

                    if ($http_code === 200 && !empty($zip_url) && strpos($zip_url, 'http') === 0) {
                        // SUCCESS -> TRANSITION TO DOWNLOAD
                        $fname = basename($zip_url);
                        $lpath = $folder . '/' . $fname;
                        
                        $fp = fopen($lpath, 'w+');
                        $ch_dl = curl_init($zip_url);
                        if ($fp === false || $ch_dl === false) {
                            if ($fp) fclose($fp);
                            if ($ch_dl) curl_close($ch_dl);
                            $slots[$slot] = null;
                            unset($active_transfers[$ch_id]);
                            $fallback_chunks[] = $data['job']['files'];
                            echo "\033[{$line}B\r\033[K {$label_num}: Local IO Error. Added to fallback.\033[{$line}A";
                        } else {
                            curl_setopt($ch_dl, CURLOPT_FILE, $fp);
                            curl_setopt($ch_dl, CURLOPT_SSL_VERIFYPEER, false);
                            curl_setopt($ch_dl, CURLOPT_FOLLOWLOCATION, true);
                            curl_setopt($ch_dl, CURLOPT_TIMEOUT, 3600);
                            curl_setopt($ch_dl, CURLOPT_USERAGENT, self::$user_agent);
                            curl_multi_add_handle($mh, $ch_dl);
                            
                            unset($active_transfers[$ch_id]);
                            $new_id = is_object($ch_dl) ? spl_object_id($ch_dl) : (int)$ch_dl;
                            $active_transfers[$new_id] = $data;
                            $active_transfers[$new_id]['state'] = 'DL';
                            $active_transfers[$new_id]['file_path'] = $lpath;
                            $active_transfers[$new_id]['file_name'] = $fname;
                            $active_transfers[$new_id]['fp'] = $fp;
                            $active_transfers[$new_id]['handle'] = $ch_dl;
                            $slots[$slot] = $new_id;
                        }

                    } else {
                        // ZIP FAILED
                        $slots[$slot] = null;
                        unset($active_transfers[$ch_id]);
                        if ($data['job']['attempts'] <= $max_retries) {
                            echo "\033[{$line}B\r\033[K {$label_num}: Zip Failed. Retrying...\033[{$line}A";
                            array_unshift($queue, $data['job']);
                            sleep(1);
                        } else {
                            $fallback_chunks[] = $data['job']['files'];
                            echo "\033[{$line}B\r\033[K {$label_num}: Zip Failed. Added to fallback.\033[{$line}A";
                        }
                    }
                    
                } elseif ($data['state'] === 'DL') {
                    // DOWNLOAD FINISHED
                    $http_code = curl_getinfo($ch, CURLINFO_HTTP_CODE);
                    $curl_error = curl_error($ch);
                    $download_size = curl_getinfo($ch, CURLINFO_SIZE_DOWNLOAD);
                    $expected_size = curl_getinfo($ch, CURLINFO_CONTENT_LENGTH_DOWNLOAD);

                    fflush($data['fp']);
                    fclose($data['fp']);
                    usleep(100000);
                    if (is_resource($ch)) curl_close($ch);
                    
                    $file_exists = file_exists($data['file_path']);
                    $file_size = $file_exists ? filesize($data['file_path']) : 0;
                    
                    if ($http_code >= 200 && $http_code < 300 && empty($curl_error) && $file_size > 0 && ($expected_size == -1 || $download_size == $expected_size)) {
                        // SUCCESS -> TRANSITION TO UNZIP
                        $active_transfers[$ch_id]['state'] = 'UNZIP';
                    } else {
                        // FAILURE
                        $slots[$slot] = null;
                        unset($active_transfers[$ch_id]);
                        if ($file_exists) unlink($data['file_path']);
                        
                        if ($data['job']['attempts'] <= $max_retries) {
                            $err_msg = empty($curl_error) ? "HTTP $http_code" : "Net Error";
                            echo "\033[{$line}B\r\033[K {$label_num}: DL Failed ($err_msg). Retrying...\033[{$line}A";
                            array_unshift($queue, $data['job']);
                        } else {
                            $fallback_chunks[] = $data['job']['files'];
                            echo "\033[{$line}B\r\033[K {$label_num}: DL Failed. Added to fallback.\033[{$line}A";
                        }
                        sleep(1);
                    }
                }
            }

        } while ($running > 0 || !empty($queue) || !empty($active_transfers));

        echo "\033[{$max_concurrency}B"; 
        echo "\033[?25h"; 
        if (is_resource($mh)) curl_multi_close($mh);

        if (!empty($fallback_chunks)) {
            echo "\nProcessing failed chunks via stream fallback...\n";
            foreach ($fallback_chunks as $chunk) {
                foreach ($chunk as $f) {
                    $ldest = $folder . '/' . $f->name;
                    $ldir = dirname($ldest);
                    if (!is_dir($ldir)) @mkdir($ldir, 0755, true);
                    try {
                        echo "   - Streaming: {$f->name}\n";
                        self::download_file_via_stream($f->name, $ldest);
                        self::$global_bytes_processed += (isset($f->size) ? (float)$f->size : 0);
                    } catch (\Exception $fe) {
                        echo "   - Failed to download: {$f->name}\n";
                    }
                }
            }
        }
    }

    /**
     * Adds a single file object to the ncdu tree by reference.
     * @param array $root The ncdu tree, passed by reference.
     * @param object $file The file object (must have ->name and ->size).
     * @param array $processed_files Pass-by-ref array for de-duplication
     * @return int Returns 1 if a duplicate was skipped, 2 if a dir (trailing /) was skipped, 3 if a dir-total (conflicting) entry was skipped, 0 otherwise.
     */
    private static function add_file_to_ncdu_tree(&$root, $file, &$processed_files) {
        
        if (empty($file->name) || !isset($file->size)) return 0;
        
        // Skip explicitly defined directories (trailing slash)
        if (substr($file->name, -1) === '/') {
            return 2;
        }

        // Deduplication
        if (isset($processed_files[$file->name])) {
            return 1;
        }
        $processed_files[$file->name] = true; 

        $path = $file->name; 
        $parts = explode('/', $path);
        $filename = array_pop($parts); 
        $file_size = (int)$file->size; 
        
        $node = &$root;
        $node['count']++;
        
        $current_parts_path = [];
        
        // Process directories in path
        foreach ($parts as $part) { 
            if (empty($part)) continue;

            if (!isset($node['children'][$part])) { 
                // Create directory node
                $node['children'][$part] = [
                    'name' => $part, 
                    'isdir' => true, 
                    'children' => [], 
                    'asize' => 0, 
                    'dsize' => 0, 
                    'count' => 0
                ];
            } else if ($node['children'][$part]['isdir'] === false) {
                // Conflict: A file exists where a directory is needed.
                // Backtrack counts for the path we just walked.
                $root['count']--;
                $temp_node = &$root;
                foreach ($current_parts_path as $parent_part) {
                    if (isset($temp_node['children'][$parent_part])) {
                        $temp_node = &$temp_node['children'][$parent_part];
                        $temp_node['count']--;
                    }
                }
                return 0;
            }

            // Descend
            $node = &$node['children'][$part];
            $node['count']++;
            $current_parts_path[] = $part;
        }
        
        // Process the final file
        if ($filename !== '') { 
            if (isset($node['children'][$filename]) && $node['children'][$filename]['isdir']) { 
                // Conflict: A directory exists where a file is needed.
                // Backtrack counts.
                $root['count']--;
                $temp_node = &$root;
                foreach ($parts as $part) {
                    if (isset($temp_node['children'][$part])) { 
                        $temp_node = &$temp_node['children'][$part];
                        $temp_node['count']--;
                    }
                }
                return 3;
            } else {
                // Add file node
                if (!isset($node['children'][$filename])) {
                    $node['children'][$filename] = [
                        'name' => $filename, 
                        'asize' => $file_size, 
                        'dsize' => $file_size, 
                        'count' => 1, 
                        'isdir' => false
                    ];
                }
            }
        }
        return 0;
    }

    /**
     * Browses a remote site's file system using ncdu.
     */
    private static function ncdu() {
        // 1. Check for ncdu dependency
        $ncdu_path = trim((string) shell_exec('command -v ncdu'));
        if (empty($ncdu_path)) {
            echo "Error: The 'ncdu' command is not found in your system's PATH.\n";
            echo "Please install ncdu (NCurses Disk Usage) to use this feature.\n";
            exit(1);
        }

        // Note: self::load_config() is already run by main()

        // 2. Backup Token Setup
        if (self::$session_id) {
            self::$backup_token = self::$session_id;
            echo "Reusing backup session: " . self::$backup_token . "\n";
        } else {
            self::$backup_token = substr(bin2hex(random_bytes(20)), 0, -28);
        }

        // 3. Fetch Manifest
        $files_manifest_chunks = [];
        try {
            if (self::$session_id) {
                echo "Fetching existing manifest...\n";
                $files_manifest_chunks = self::make_request('/manifest', [], 'GET');
            } else {
                echo "Generating new file manifest (this may take a while)...\n";
                // runManifestGeneration uses self::$exclude_paths automatically
                $files_manifests = self::runManifestGeneration(false);
                $files_manifest_chunks = $files_manifests;
            }
            
            if (!is_array($files_manifest_chunks)) {
                throw new \Exception("Could not fetch or parse manifest. Is the session ID valid?");
            }
        } catch (\Exception $e) {
            echo "Error: {$e->getMessage()}\n";
            if (!self::$session_id) self::cleanupTemporaryFiles();
            exit(1);
        }

        // 4. Download, Process, and Build Tree
        echo "Processing remote file manifest and building tree...\n";
        $ncdu_tree = ['name' => '/', 'isdir' => true, 'children' => [], 'asize' => 0, 'dsize' => 0, 'count' => 0];
        $processed_files = [];

        if (self::$debug) {
            echo "Debug mode enabled, will aggregate files in memory for manifest export.\n";
        }

        // Fetch Parallel - fetch_manifests_parallel uses self::$exclude_paths internally
        list($remote_files_map, $all_remote_files_debug) = self::fetch_manifests_parallel($files_manifest_chunks);
        
        $duplicate_count = 0;
        $skipped_slash_dir_count = 0;
        $skipped_total_dir_count = 0;

        foreach ($remote_files_map as $file) {
             $result = self::add_file_to_ncdu_tree($ncdu_tree, $file, $processed_files);
             if ($result === 1) $duplicate_count++;
             else if ($result === 2) $skipped_slash_dir_count++;
             else if ($result === 3) $skipped_total_dir_count++;
        }

        if ($skipped_slash_dir_count > 0) echo "Ignored " . number_format($skipped_slash_dir_count) . " directory (trailing /) entries.\n";
        if ($skipped_total_dir_count > 0) echo "Ignored " . number_format($skipped_total_dir_count) . " conflicting directory-total entries.\n";
        if ($duplicate_count > 0) echo "Ignored " . number_format($duplicate_count) . " duplicate file entries.\n";
        
        if (self::$debug) {
            $manifest_path = getcwd() . '/.disembark-ncdu-manifest.json';
            file_put_contents($manifest_path, json_encode($all_remote_files_debug, JSON_PRETTY_PRINT));
            echo "Debug: Saved raw manifest to $manifest_path\n";
        }

        // 5. Build the ncdu Tree
        echo "Directory tree build complete.\n";
        $ncdu_json_array = self::format_ncdu_json_array($ncdu_tree);

        // Wrap in the ncdu shell format
        $ncdu_output_data = [
            1, 0, // ncdu JSON format version
            ["progname" => "disembark-cli", "progver" => self::VERSION],
            $ncdu_json_array // The actual tree
        ];
        $json_string = json_encode($ncdu_output_data);

        if (self::$debug) {
            $input_path = getcwd() . '/.disembark-ncdu-input.json';
            file_put_contents($input_path, $json_string);
            echo "Debug: Saved final ncdu input to $input_path\n";
        }

        // 6. Launch ncdu
        echo "Launching ncdu...\n";
        $descriptors = [
            0 => ["pipe", "r"], // stdin
            1 => STDOUT,        // stdout
            2 => STDERR         // stderr
        ];
        $process = proc_open('ncdu -f -', $descriptors, $pipes);
        if (is_resource($process)) {
            fwrite($pipes[0], $json_string);
            fclose($pipes[0]);
            proc_close($process);
        } else {
            echo "Error: Failed to launch ncdu process.\n";
        }

        // 7. Final Cleanup
        if (!self::$session_id) {
            self::cleanupTemporaryFiles();
        }
    }

    /**
     * Displays information about the connected site.
     */
    private static function info() {
        echo "Fetching site information for " . self::$siteUrl . "...\n";

        // Default states
        $status_label = "Unknown";
        $temp_usage = "N/A";
        $db_stats = "N/A";
        $row_stats = "N/A";
        $token_display = self::$token;
        $scan_info = "No recent scan found";
        $session_output = "";

        try {
            // 1. Check Connection & Temp Storage via /backup-size
            $size_response = self::make_request('/backup-size', [], 'GET');
            
            if (isset($size_response->size)) {
                $status_label = "Connected (Valid Credentials)";
                $temp_usage = self::humanFilesize($size_response->size);

                if (self::$cleanup) {
                    try {
                        self::make_request('/cleanup', [], 'GET');
                        $temp_usage .= " (purged)";
                    } catch (\Exception $e) {
                        $temp_usage .= " (purge failed)";
                    }
                }

                // Process Cached Scan Stats
                if (isset($size_response->scan_stats) && is_object($size_response->scan_stats)) {
                    $stats = $size_response->scan_stats;
                    $scan_size = self::humanFilesize($stats->total_size);
                    $scan_count = number_format($stats->total_files);
                    
                    // Format timestamp (try to use system date for consistent timezone)
                    $format = "'+%b %e, %Y %l:%M %p %Z'"; 
                    $scan_time = "";
                    
                    if (PHP_OS === 'Darwin') {
                        $cmd = "date -r {$stats->timestamp} $format";
                        $scan_time = trim((string)shell_exec($cmd));
                    } elseif (strtoupper(substr(PHP_OS, 0, 3)) === 'LIN') {
                        $cmd = "date -d @{$stats->timestamp} $format";
                        $scan_time = trim((string)shell_exec($cmd));
                    }

                    if (empty($scan_time)) {
                        $scan_time = date('M j, Y g:i A T', $stats->timestamp);
                    }
                    
                    $time_diff = time() - $stats->timestamp;
                    $age_str = ($time_diff < 60) ? "Just now" :
                               (($time_diff < 3600) ? floor($time_diff/60) . "m ago" : 
                               (($time_diff < 86400) ? floor($time_diff/3600) . "h ago" : floor($time_diff/86400) . "d ago"));

                    $scan_info = "$scan_count files ($scan_size)\n";
                    $scan_info .= sprintf("%-20s %s (%s)", "", "scanned $scan_time", $age_str);
                }

                if (isset($size_response->sessions) && is_array($size_response->sessions) && !empty($size_response->sessions)) {
                    $session_output .= "\nPrevious Sessions (Resume via --session-id)\n";
                    $session_output .= "----------------------------------------\n";
                    
                    foreach ($size_response->sessions as $session) {
                        // Use PHP date for list speed
                        $date_str = date('M j, Y g:i A', $session->timestamp); 
                        $id_short = $session->token;
                        $chunks   = $session->count;
                        
                        $session_output .= sprintf("%-20s %-16s (%d chunks)\n", $date_str, $id_short, $chunks);
                    }
                }

            } else {
                $status_label = "Connected (Unexpected Response)";
            }

            // 2. Fetch Database Stats
            $db_response = self::make_request('/database', [], 'GET');
            if (is_array($db_response)) {
                 $table_count = count($db_response);
                 $total_bytes = array_sum(array_column($db_response, 'size'));
                 $total_rows = array_sum(array_column($db_response, 'row_count'));
                 
                 $db_stats = number_format($table_count) . " tables (" . self::humanFilesize($total_bytes) . ")";
                 $row_stats = number_format($total_rows) . " rows";
            }

        } catch (\Exception $e) {
            $msg = $e->getMessage();
            if (strpos($msg, '403') !== false) {
                $status_label = "Invalid Credentials (403 Forbidden)";
            } elseif (strpos($msg, '404') !== false) {
                 $status_label = "Plugin not found or API disabled (404)";
            } else {
                $status_label = "Failed";
            }
        }

        // Display Output
        echo "\nSite Information\n";
        echo "----------------\n";
        printf("%-20s %s\n", "Site URL:", self::$siteUrl);
        printf("%-20s %s\n", "Token:", $token_display);
        printf("%-20s %s\n", "Connection Status:", $status_label);
        echo "\nStats\n";
        echo "----------------\n";
        printf("%-20s %s\n", "Temp Storage Used:", $temp_usage);
        printf("%-20s %s\n", "File System:", $scan_info);
        printf("%-20s %s\n", "Database Size:", $db_stats);
        printf("%-20s %s\n", "Total Rows:", $row_stats);
        
        if (!empty($session_output)) {
            echo $session_output;
        }
        
        echo "\n";
    }

    /**
     * Builds an aggregated associative tree from a flat file list for ncdu.
     */
    private static function build_ncdu_tree($all_remote_files) {
        $root = ['name' => '/', 'isdir' => true, 'children' => [], 'asize' => 0, 'dsize' => 0, 'count' => 0];
        foreach ($all_remote_files as $file) {
            if (empty($file->name) || !isset($file->size)) continue;
            
            $path = rtrim($file->name, '/'); // Keep this fix

            $parts = explode('/', $path);
            $filename = array_pop($parts);
            $file_size = (int)$file->size;
            
            $node = &$root;
            // Add file size all the way up the tree
            $node['asize'] += $file_size;
            $node['dsize'] += $file_size;
            $node['count']++;
            
            foreach ($parts as $part) {
                if (empty($part)) continue;
                if (!isset($node['children'][$part])) {
                    // Create dir node if it doesn't exist
                    $node['children'][$part] = ['name' => $part, 'isdir' => true, 'children' => [], 'asize' => 0, 'dsize' => 0, 'count' => 0];
                }
                $node = &$node['children'][$part];
                // Add file size to this dir node
                $node['asize'] += $file_size;
                $node['dsize'] += $file_size;
                $node['count']++;
            }
            
            // Use ($filename !== '') instead of !empty() to correctly handle files named "0"
            if ($filename !== '') {
                // Add the file node
                // Check if a directory with this name already exists (conflicting path)
                if (isset($node['children'][$filename]) && $node['children'][$filename]['isdir']) {
                    // A directory already exists, this is a conflicting file.
                    // For ncdu's purpose, we'll log it as a special file inside.
                    $conflicting_name = $filename . ' (file)';
                    $node['children'][$conflicting_name] = ['name' => $conflicting_name, 'asize' => $file_size, 'dsize' => $file_size, 'count' => 1, 'isdir' => false];
                } else {
                    // This is the normal case: add the file node
                    $node['children'][$filename] = ['name' => $filename, 'asize' => $file_size, 'dsize' => $file_size, 'count' => 1, 'isdir' => false];
                }
            }
        }
        return $root;
    }

    /**
     * Recursively formats the associative tree into ncdu's expected JSON array format.
     */
    private static function format_ncdu_json_array($node) {
        // 1. Create the node metadata object
        $ncdu_node = [
            'name' => $node['name'],
            'asize' => $node['asize'],
            'dsize' => $node['dsize'],
            'count' => $node['count'],
            'isdir' => $node['isdir']
        ];

        // 2. If it's a FILE, just return the object.
        if (!$node['isdir']) {
            return $ncdu_node;
        }

        // 3. If it's a DIRECTORY, create an array.
        $output = [$ncdu_node]; // Start the array with the dir metadata

        // 4. Process children
        if (!empty($node['children'])) {
            $children = array_values($node['children']);
            
            // Sort children: directories first, then alphabetically
            usort($children, function ($a, $b) {
                if ($a['isdir'] !== $b['isdir']) {
                    return $a['isdir'] ? -1 : 1; // Dirs first
                }
                return strcmp($a['name'], $b['name']); // Then by name
            });
            
            // 5. Add recursive results to the output array
            foreach ($children as $child) {
                // This will append EITHER an OBJECT (for a file) OR an ARRAY (for a dir)
                $output[] = self::format_ncdu_json_array($child); 
            }
        }
        
        // 6. Return the full directory array
        return $output;
    }

    /**
     * Initiates a full backup of the connected site.
     */
    private static function backup() {

        // 1. Setup Session / Backup Token
        if (self::$session_id) {
            self::$backup_token = self::$session_id;
            echo "Reusing backup session: " . self::$backup_token . "\n";
        } else {
            self::$backup_token = substr(bin2hex(random_bytes(20)), 0, -28);
        }

        $files = [];
        $filtered_files = [];
        $client_side_filtering = false;
        // 2. Manifest Logic
        if (self::$session_id) {
            try {
                echo "Fetching existing manifest...\n";
                $files = self::make_request('/manifest', [], 'GET');
                if (!is_array($files)) {
                    throw new \Exception("Could not fetch or parse existing manifest.");
                }

                // Handle Client-side filtering if exclusions exist on a reused session
                if (!empty(self::$exclude_paths)) {
                    $client_side_filtering = true;
                    echo "Applying client-side exclusions to session manifest...\n";
                    
                    $temp_manifest_dir = rtrim(sys_get_temp_dir(), DIRECTORY_SEPARATOR) . '/disembark-manifests-' . self::$backup_token;
                    if (!is_dir($temp_manifest_dir) && !mkdir($temp_manifest_dir, 0755, true)) {
                        throw new \Exception("Could not create temp manifest dir for filtering.");
                    }

                    $all_remote_files = [];
                    foreach ($files as $file_manifest) {
                        $local_chunk_path = $temp_manifest_dir . '/' . $file_manifest->name;
                        self::download_file_direct($file_manifest->url, $local_chunk_path);
                        $chunk_json = file_get_contents($local_chunk_path);
                        unlink($local_chunk_path);
                        $chunk_files = json_decode($chunk_json);
                        if (is_array($chunk_files)) {
                            $all_remote_files = array_merge($all_remote_files, $chunk_files);
                        }
                    }
                    self::delete_directory($temp_manifest_dir);
                    foreach ($all_remote_files as $file) {
                        if (!is_object($file) || empty($file->name)) continue;
                        $is_excluded = false;
                        foreach (self::$exclude_paths as $exclude_path) {
                            if ($file->name === $exclude_path || strpos($file->name, $exclude_path . '/') === 0) {
                                $is_excluded = true;
                                break;
                            }
                        }

                        if (!$is_excluded) {
                            $filtered_files[] = $file;
                        }
                    }
                }
                
                self::displayBackupSummary($client_side_filtering ? [] : $files);
            } catch (\Exception $e) {
                echo "Error fetching manifest: {$e->getMessage()}\n";
                exit(1);
            }
        } else {
            // New Session
            if (!self::$skip_files) {
                // Pass the static preview flag to runPreview
                $files = self::runPreview(self::$preview);
            } else {
                self::displayBackupSummary([]);
                echo "Skipping file analysis as requested.\n";
            }
        }

        if (!empty(self::$exclude_paths)) {
            echo "Excluding the following paths during backup:\n";
            foreach (self::$exclude_paths as $path) echo " - $path\n";
        }
        if (!empty(self::$exclude_tables)) {
            echo "Excluding database tables matching:\n";
            foreach (self::$exclude_tables as $pattern) echo " - $pattern\n";
        }

        echo "Starting backup for " . self::$siteUrl . " using backup token " . self::$backup_token . "\n";
        
        $temp_directory_name = "snapshot-" . time();
        $temp_directory = getcwd() . DIRECTORY_SEPARATOR . $temp_directory_name;
        $local_db_path = $temp_directory . '/database';
        $local_public_path = $temp_directory . '/public';
        
        $domain = preg_replace('/^https?:\/\/(www\.)?/', '', self::$siteUrl);
        $domain = preg_replace('/\//', '_', $domain);
        $final_zip_name = "{$temp_directory_name}-{$domain}.zip";

        // STATS COUNTERS
        $total_items_processed = 0;

        try {
            if (!mkdir($temp_directory, 0755, true)) throw new \Exception("Could not create temp directory $temp_directory.");
            if (!mkdir($local_public_path, 0755, true)) throw new \Exception("Could not create public directory.");
            // --- CALCULATE GLOBAL TOTALS ---
            $total_db_bytes = 0;
            $database = [];

            // Pre-fetch DB info to calculate total size
            if (!self::$skip_db) {
                $all_tables = self::make_request('/database', [], 'GET');
                if (!is_array($all_tables)) $all_tables = [];

                if (!empty(self::$exclude_tables)) {
                    foreach ($all_tables as $table) {
                        $is_excluded = false;
                        foreach (self::$exclude_tables as $pattern) {
                            if (fnmatch($pattern, $table->table)) {
                                $is_excluded = true;
                                break;
                            }
                        }
                        if (!$is_excluded) $database[] = $table;
                    }
                } else {
                    $database = $all_tables;
                }

                foreach ($database as $tbl) {
                    $total_db_bytes += (isset($tbl->size) ? (float)$tbl->size : 0);
                }
            }
            
            $total_file_bytes = 0;
            if (!self::$skip_files) {
                if (!empty($filtered_files)) {
                     foreach ($filtered_files as $f) {
                         $total_file_bytes += (isset($f->size) ? (float)$f->size : 0);
                         $total_items_processed++;
                     }
                } elseif (!empty($files)) {
                    // $files contains manifest chunks with a 'size' property
                    foreach ($files as $chunk) {
                        $total_file_bytes += isset($chunk->size) ? (float)$chunk->size : 0;
                        $total_items_processed += isset($chunk->count) ? (int)$chunk->count : 0;
                    }
                }
            }

            self::$global_bytes_total = $total_db_bytes + $total_file_bytes;
            self::$global_bytes_processed = 0;
            // -------------------------------

            if (!self::$skip_db) {
                if (!mkdir($local_db_path, 0755, true)) throw new \Exception("Could not create db directory.");
                $db_export_file = $local_public_path . '/database-' . $temp_directory_name . '.sql';
                file_put_contents($db_export_file, "/*!40101 SET NAMES utf8mb4 */;\nSET sql_mode='NO_AUTO_VALUE_ON_ZERO';\n");
                if (!empty($database)) {
                    // This calls download_db_file -> download_file_direct, which updates global progress
                    self::processDatabaseBackup($database, $db_export_file, $local_db_path);
                    $total_items_processed += count($database); // Add tables to item count
                } else {
                    echo "No database tables to back up.\n";
                }
                rmdir($local_db_path);
            } else {
                echo "Skipping database backup as requested.\n";
            }

            if (!self::$skip_files) {
                echo "Preparing optimized file backup...\n";
                $all_files_to_process = $filtered_files;
                if (empty($all_files_to_process) && !empty($files)) {
                    // Fetch using parallel fetcher (uses static excludes)
                    list($remote_files_map, $debug) = self::fetch_manifests_parallel($files);
                    $all_files_to_process = array_values($remote_files_map);
                    // Update exact count from fetch
                    $total_items_processed = count($remote_files_map) + (!self::$skip_db ? count($database) : 0);
                }

                if (!empty($all_files_to_process)) {
                    $large_files_queue = [];
                    $small_files_queue = [];

                    foreach ($all_files_to_process as $file) {
                        $fsize = isset($file->size) ? (int)$file->size : 0;
                        if ($fsize > self::$large_file_threshold) {
                            $large_files_queue[] = $file;
                        } else {
                            $small_files_queue[] = $file;
                        }
                    }

                    if (!empty($large_files_queue)) {
                         self::download_files_parallel($large_files_queue, $local_public_path);
                    }

                    if (!empty($small_files_queue)) {
                        $file_chunks = [];
                        $current_chunk = [];
                        $current_chunk_size = 0;

                        foreach ($small_files_queue as $file) {
                            $fsize = $file->size ?? 0;
                            if (($current_chunk_size + $fsize > self::$file_chunk_max_size) || (count($current_chunk) >= self::$file_chunk_size)) {
                                $file_chunks[] = $current_chunk;
                                $current_chunk = [$file];
                                $current_chunk_size = $fsize;
                            } else {
                                $current_chunk[] = $file;
                                $current_chunk_size += $fsize;
                            }
                        }
                        if (!empty($current_chunk)) $file_chunks[] = $current_chunk;
                        
                        if (!empty($file_chunks)) {
                            self::sync_chunks_parallel($file_chunks, $local_public_path);
                        }

                        $wait_ticks = 0;
                        while (count(glob($local_public_path . '/sync-files-*.zip')) > 0 || count(glob($local_public_path . '/files-*.zip')) > 0) {
                            if ($wait_ticks === 0) echo "Waiting for background extraction to finish...";
                            echo ".";
                            sleep(1);
                            $wait_ticks++;
                            if ($wait_ticks > 120) { 
                                 echo "\nWarning: Background extraction taking too long. Some zip files may be left in the backup.\n";
                                 break;
                            }
                        }
                        if ($wait_ticks > 0) echo " Done.\n";
                        echo "\n";
                        echo "Small file batches complete.\n";
                    }
                }
            } else {
                echo "Skipping file backup as requested.\n";
            }

            echo "Generating final local zip file: $final_zip_name...\n";
            self::zip_directory($local_public_path, $final_zip_name);

        } catch (\Exception $e) {
            echo "\nBackup failed: {$e->getMessage()}\n";
            echo "Partial files may be left in {$temp_directory}\n";
            if (!self::$session_id) self::cleanupTemporaryFiles();
            exit(1);
        }

        try {
            self::delete_directory($temp_directory);
        } catch (\Exception $e) { }

        if (!self::$session_id) {
            self::cleanupTemporaryFiles();
        } else {
            echo "\nSkipping remote cleanup to preserve UI session files.\n";
        }

        // FINAL SUMMARY
        $final_size = file_exists($final_zip_name) ? filesize($final_zip_name) : 0;
        
        echo "\nBackup Completed Successfully\n";
        echo "-----------------------------\n";
        printf("%-15s %s\n", "File Name:", $final_zip_name);
        printf("%-15s %s\n", "Total Size:", self::humanFilesize($final_size));
        printf("%-15s %s\n", "Items:", number_format($total_items_processed));
        echo "\n";
    }

    private static function sites() {
        $homeDir = getenv('HOME');
        if (!$homeDir) {
            echo "Error: Unable to determine the home directory.\n";
            exit(1);
        }

        $filePath = $homeDir . '/.disembark';
        $data = [];
        if (file_exists($filePath)) {
            $jsonContents = file_get_contents($filePath);
            $data = json_decode($jsonContents);
            if (!is_array($data)) {
                if (is_object($data)) {
                    $data = [$data];
                } else {
                    echo "Error: Invalid data in $filePath.\n";
                    exit(1);
                }
            }
        } else {
            echo "No configuration file found at $filePath. No sites connected.\n";
            exit(0);
        }

        if (empty($data)) {
            echo "No sites connected.\n";
        } else {
            echo "Connected sites:\n";
            foreach ($data as $entry) {
                if (is_object($entry) && isset($entry->siteUrl)) {
                    echo "- {$entry->siteUrl}\n";
                } else {
                    echo "- (Invalid entry in config file)\n";
                }
            }
        }
    }

    private static function version() {
        $version = self::VERSION;
        echo "Disembark CLI v{$version}\n";
    }

    private static function upgrade() {
        echo "Checking for updates...\n";
        $current_version = self::VERSION;

        $latest_version_cmd = "curl -sL -o /dev/null -w '%{url_effective}' https://github.com/DisembarkHost/disembark-cli/releases/latest | sed 's#.*/v##'";
        $latest_version = trim(shell_exec($latest_version_cmd));
        if (empty($latest_version) || !preg_match('/^\d+\.\d+\.\d+/', $latest_version)) {
            echo "Error: Could not fetch the latest version information or invalid version format received ('{$latest_version}').\n";
            exit(1);
        }

        if (version_compare($latest_version, $current_version, '>')) {
            echo "New version {$latest_version} available. Upgrading from {$current_version}...\n";
            $pharPath = \Phar::running(false);
            if (empty($pharPath)) {
                $scriptPath = realpath($_SERVER['argv'][0]);
                if ($scriptPath && is_file($scriptPath)) {
                    $pharPath = $scriptPath;
                    echo "Info: Determined script path as {$pharPath}.\n";
                } else {
                    echo "Error: Could not determine the path of the running script. Upgrade failed.\n";
                    echo "Please download the latest version manually from:\n";
                    echo "https://github.com/DisembarkHost/disembark-cli/releases/latest/download/disembark.phar\n";
                    exit(1);
                }
            }

            $downloadUrl = 'https://github.com/DisembarkHost/disembark-cli/releases/latest/download/disembark.phar';
            $tmpFile = tempnam(sys_get_temp_dir(), 'disembark-upgrade-');
            if ($tmpFile === false) {
                echo "Error: Could not create a temporary file. Upgrade failed.\n";
                exit(1);
            }

            echo "Downloading new version from {$downloadUrl}...\n";
            $download_cmd = sprintf("curl -sL '%s' -o '%s'", $downloadUrl, $tmpFile);
            shell_exec($download_cmd);
            if (!file_exists($tmpFile) || filesize($tmpFile) === 0) {
                echo "Error: Failed to download the new version. File is empty or does not exist at {$tmpFile}. Upgrade failed.\n";
                if (file_exists($tmpFile)) unlink($tmpFile);
                exit(1);
            }

            chmod($tmpFile, 0755);
            $move_cmd = sprintf("mv '%s' '%s'", $tmpFile, $pharPath);
            exec($move_cmd, $output, $return_var);
            if ($return_var !== 0) {
                echo "Error: Failed to replace the current script at {$pharPath}.\n";
                echo "You might need to run the command with sufficient permissions (e.g., using sudo):\n";
                echo "sudo disembark upgrade\n";
                if (file_exists($tmpFile)) {
                    unlink($tmpFile);
                }
                exit(1);
            }


            echo "Upgrade complete. You are now on version {$latest_version}.\n";
        } else {
            echo "You are already using the latest version ({$current_version}).\n";
        }
    }

    private static function showHelp() {
        echo "Disembark CLI\n";
        echo "\n";
        echo "Usage:\n";
        echo "  disembark backup <site-url> [options]\n";
        echo "  disembark sync <site-url> [<folder>] [options]\n";
        echo "  disembark <command>\n";
        echo "\n";
        echo "Primary Commands:\n";
        echo "\n";
        echo "  disembark backup <site-url> [options]\n";
        echo "    Initiate a backup for a connected site.\n";
        echo "\n";
        echo "    Options for backup:\n";
        echo "      --preview                          Show a summary of files and DB tables to be backed up without running the backup.\n";
        echo "      --skip-db                          Skip the database backup and only process files.\n";
        echo "      --skip-files                       Skip the file backup and only process the database.\n";
        echo "      --skip-checksums                   Skip file integrity checks. Only downloads files missing from destination.\n";
        echo "      -x <path>                          Exclude a file or directory path (e.g., wp-content/cache). Can be specified multiple times.\n";
        echo "      --exclude-tables=<tables>          Exclude specific database tables (comma-separated, no spaces). Wildcards (*) are supported.\n";
        echo "      --db-max-size=<size>               Set max DB part size (e.g., 25MB, 1G). Default: 200MB.\n";
        echo "      --db-max-rows=<num>                Set max DB part rows (e.g., 100000). Default: 1000000.\n";
        echo "      --large-file-threshold=<size>      Set size threshold for individual file downloads (e.g., 100MB). Default: 200MB.\n";
        echo "      --session-id=<id>                  Reuse a specific backup session ID (backup token) generated by the plugin UI.\n";
        echo "\n";
        echo "  disembark sync <site-url> [<folder>] [options]\n";
        echo "    Create or update a local mirror of the site.\n";
        echo "\n";
        echo "    Options for sync:\n";
        echo "      --debug                            Save local and remote manifest files for debugging.\n";
        echo "      --skip-db                          Skip the database sync and only process files.\n";
        echo "      --skip-files                       Skip the file sync and only process the database.\n";
        echo "      --db-max-size=<size>               Set max DB part size (e.g., 25MB, 1G). Default: 200MB.\n";
        echo "      --db-max-rows=<num>                Set max DB part rows (e.g., 100000). Default: 1000000.\n";
        echo "      --file-chunk-size=<num>            Set number of files per sync zip. Default: 2500.\n";
        echo "      --file-chunk-max-size=<size>       Set max total size per sync zip (e.g., 100MB). Default: 500MB.\n";
        echo "      --large-file-threshold=<size>      Set size threshold for individual file downloads (e.g., 100MB). Default: 200MB.\n";
        echo "      --session-id=<id>                  Reuse a backup session ID from the UI.\n";
        echo "\n";
        echo "  disembark ncdu <site-url> [options]\n";
        echo "    Browse the remote site's file system interactively using ncdu.\n";
        echo "\n";
        echo "    Options for ncdu:\n";
        echo "      --session-id=<id>                  Reuse a backup session ID from the UI to load its manifest.\n";
        echo "      --debug                            Save raw manifest and ncdu input files locally for debugging.\n";
        echo "\n";
        echo "  disembark info <site-url>\n";
        echo "    Display connection status, token, and storage usage for a site.\n";
        echo "\n";
        echo "Other Commands:\n";
        echo "  disembark connect <site-url> <token>   Connect to a site and save its token.\n";
        echo "  disembark remove <site-url>            Remove a connected site and its credentials.\n";
        echo "  disembark list                         List all sites currently connected.\n";
        echo "  disembark upgrade                      Upgrade the Disembark CLI tool to the latest version.\n";
        echo "  disembark version                      Show the current version of the Disembark CLI tool.\n";
        echo "\n";
        echo "Example:\n";
        echo "  disembark backup https://example.com --preview -x wp-content/uploads/large-dir --exclude-tables=wp_options,wp_logs\n";
        echo "  disembark backup https://example.com --session-id=a1b2c3d4e5f6\n";
        echo "  disembark sync https://example.com --session-id=a1b2c3d4e5f6 --skip-db\n";
    }

    private static function load_config($url) {
        // Standardize URL
        self::$siteUrl = rtrim($url, '/');
        if (!preg_match('/^https?:\/\//', self::$siteUrl)) {
            self::$siteUrl = "https://" . self::$siteUrl;
        }

        // Find Token
        $homeDir = getenv('HOME');
        if (!$homeDir) {
            echo "Error: Unable to determine the home directory.\n";
            exit(1);
        }

        $filePath = $homeDir . '/.disembark';
        if (!file_exists($filePath)) {
            echo "Error: No configuration file found at $filePath.\n";
            exit(1);
        }

        $data = json_decode(file_get_contents($filePath));
        if (!is_array($data)) $data = is_object($data) ? [$data] : [];

        foreach ($data as $entry) {
            if (is_object($entry) && isset($entry->siteUrl) && $entry->siteUrl === self::$siteUrl) {
                self::$token = $entry->token;
                return;
            }
        }

        echo "Error: No token found for " . self::$siteUrl . ".\n";
        exit(1);
    }

    private static function convert_to_bytes($size_str) {
        $size_str = strtolower(trim($size_str));
        
        // Use preg_match to separate the number and the unit
        preg_match('/^([0-9\.]+)\s*(kb|mb|gb|k|m|g)?$/', $size_str, $matches);
        
        $value = isset($matches[1]) ? (float) $matches[1] : 0;
        $unit = isset($matches[2]) ? $matches[2] : '';

        switch ($unit) {
            case 'g':
            case 'gb':
                $value *= 1024 * 1024 * 1024;
                break;
            case 'm':
            case 'mb':
                $value *= 1024 * 1024;
                break;
            case 'k':
            case 'kb':
                $value *= 1024;
                break;
        }
        return (int) $value;
    }

    /**
     * Convert bytes to human-readable format.
     */
    private static function humanFilesize($bytes, $decimals = 2) {
        if ($bytes === null) return '0 B';
        if (!is_numeric($bytes)) return 'N/A';

        $bytesNum = (float) $bytes;

        if ($bytesNum == 0) return '0 B';

        $factor = floor(log($bytesNum, 1024));
        $sz = 'BKMGTP';
        $max_factor = strlen($sz) - 1;

        $factor = max(0, min((int)$factor, $max_factor));

        $size_str = sprintf("%.{$decimals}f", $bytesNum / pow(1024, $factor));
        $unit = $sz[$factor];
        $suffix = ($factor > 0) ? 'B' : '';

        return $size_str . ' ' . $unit . $suffix;
    }

    /**
     * Downloads a file using native cURL.
     * Supports standard verbose output or a custom progress bar label.
     */
    private static function download_file_direct($url, $destination, $progress_label = null) {
        $attempts = 0;
        $max_attempts = 5;
        $last_error = '';

        while ($attempts < $max_attempts) {
            $attempts++;
            $fp = false;
            $ch = false;
            
            $previous_downloaded = 0;
            $has_printed_lines = false;

            try {
                $fp = fopen($destination, 'w+');
                if ($fp === false) throw new \Exception("Could not open $destination");

                $ch = curl_init($url);
                curl_setopt($ch, CURLOPT_FILE, $fp);
                curl_setopt($ch, CURLOPT_TIMEOUT, 3600);
                curl_setopt($ch, CURLOPT_SSL_VERIFYPEER, false);
                if (self::$cookie) {
                    curl_setopt($ch, CURLOPT_COOKIE, self::$cookie);
                }
                curl_setopt($ch, CURLOPT_FOLLOWLOCATION, true);
                curl_setopt($ch, CURLOPT_USERAGENT, self::$user_agent);
                curl_setopt($ch, CURLOPT_NOPROGRESS, false);
                curl_setopt($ch, CURLOPT_PROGRESSFUNCTION, function(
                    $resource, $download_size, $downloaded, $upload_size, $uploaded
                ) use ($attempts, $max_attempts, &$previous_downloaded, &$has_printed_lines, $progress_label) {
                    
                    $percent = ($download_size > 0) ? round(($downloaded / $download_size) * 100) : 0;
                    
                    // Global Stats Update
                    if (self::$global_bytes_total > 0) {
                        $delta = $downloaded - $previous_downloaded;
                        if ($delta > 0) {
                            self::$global_bytes_processed += $delta;
                            $previous_downloaded = $downloaded;
                        }
                    }

                    // Prepare Global Line (Footer)
                    $global_line = "";
                    if (self::$global_bytes_total > 0) {
                        $g_cur = self::humanFilesize(self::$global_bytes_processed);
                        $g_tot = self::humanFilesize(self::$global_bytes_total);
                        $g_perc = (self::$global_bytes_total > 0) ? round((self::$global_bytes_processed / self::$global_bytes_total) * 100) : 0;
                        $global_line = "$g_cur / $g_tot ($g_perc%)";
                    }

                    // Prepare File Line (Main Output)
                    $file_line = "";
                    if ($progress_label) {
                        // --- BAR MODE ---
                        // Pad label to align bars (adjust 60 to fit your preferred width)
                        $padded_label = str_pad($progress_label, 60);
                        $bar = self::draw_progress_bar($downloaded, $download_size, 15);
                        $size_str = self::humanFilesize($downloaded);
                        $file_line = "\r\033[K{$padded_label} {$bar} {$size_str}";
                    } else {
                        // --- STANDARD MODE ---
                        $d_human = self::humanFilesize($downloaded);
                        $t_human = ($download_size > 0) ? self::humanFilesize($download_size) : 'Unknown';
                        $prefix = ($attempts > 1) ? "[Retry $attempts] " : "";
                        $file_line = "\r\033[K     - {$prefix}Downloading: $d_human / $t_human ($percent%)...";
                    }

                    // Render
                    echo $file_line;

                    if ($global_line) {
                        echo "\n\033[K" . $global_line; 
                        echo "\033[1A"; 
                    }

                    $has_printed_lines = true;
                });

                $data = curl_exec($ch);
                $error = curl_error($ch);
                $statusCode = curl_getinfo($ch, CURLINFO_HTTP_CODE);
                
                if (is_resource($ch)) curl_close($ch);
                if (is_resource($fp)) fclose($fp);

                if (!$data) throw new \Exception("cURL Error: $error");
                if ($statusCode >= 400) throw new \Exception("HTTP Error $statusCode");

                // Cleanup Output
                if ($has_printed_lines) {
                    if ($progress_label) {
                        // Bar Mode: Move to next line (footer line).
                        echo "\n";
                    } else {
                        // Standard Mode: Replace progress with "Complete" message
                        echo "\r\033[K";
                        echo "     - Download complete.\n";
                        // Restore global footer logic
                        if (self::$global_bytes_total > 0) {
                            $g_cur = self::humanFilesize(self::$global_bytes_processed);
                            $g_tot = self::humanFilesize(self::$global_bytes_total);
                            $g_perc = round((self::$global_bytes_processed / self::$global_bytes_total) * 100);
                            echo "\033[K$g_cur / $g_tot ($g_perc%)\r";
                        } else {
                            echo "\r\033[K\033[1A";
                        }
                    }
                } else if ($progress_label) {
                     echo "\n";
                } else {
                    echo "     - Download complete.\n";
                }
                return;

            } catch (\Exception $e) {
                if (is_resource($ch)) curl_close($ch);
                if (is_resource($fp)) fclose($fp);
                if ($has_printed_lines) {
                    // Cleanup visual artifacts before printing error
                     if ($progress_label) echo "\n\033[K"; 
                     else echo "\n\r\033[K\033[1A\r\033[K";
                }

                if ($attempts < $max_attempts) {
                    echo "     - Warning: Download failed: {$e->getMessage()}. Retrying...\n";
                    sleep(2);
                } else {
                    $last_error = $e->getMessage();
                }
            }
        }
        throw new \Exception("Download failed: $last_error");
    }

    /**
     * Downloads large files using a rolling queue with Multi-line Progress Bars + Global Footer.
     * Includes size validation and fallback to stream API.
     */
    private static function download_files_parallel($files, $folder) {
        $max_concurrency = 4;
        $max_retries = 3; 
        $mh = curl_multi_init();

        $queue = [];
        foreach ($files as $f) {
            $queue[] = ['file' => $f, 'attempts' => 0];
        }

        $active_transfers = []; 
        $slots = array_fill(0, $max_concurrency, null); 
        $total_files = count($files);
        $processed_count = 0;
        $fallback_queue = []; 

        // Hide Cursor
        echo "\033[?25l";
        echo " - Starting threaded download of {$total_files} large files...\n";
        // Reserve space: Concurrency lines + 1 extra line for Global Footer
        echo str_repeat("\n", $max_concurrency + 1);
        echo "\033[" . ($max_concurrency + 1) . "A"; // Move cursor up

        do {
            // 1. REFILL QUEUE
            while (count($active_transfers) < $max_concurrency && !empty($queue)) {
                $slot_index = array_search(null, $slots);
                if ($slot_index === false) break;

                $item = array_shift($queue);
                $file = $item['file'];
                $item['attempts']++;

                $local_dest = $folder . '/' . $file->name;
                $local_dir = dirname($local_dest);
                if (!is_dir($local_dir)) @mkdir($local_dir, 0755, true);
                
                $path_parts = explode('/', $file->name);
                $encoded_parts = array_map('rawurlencode', $path_parts);
                $url = self::$siteUrl . '/' . implode('/', $encoded_parts);
                
                $fp = fopen($local_dest, 'w+');
                if (!$fp) {
                    $line_offset = $slot_index + 1;
                    echo "\033[{$line_offset}B\033[KError: Could not open $local_dest\r\033[{$line_offset}A";
                    $processed_count++;
                    continue;
                }

                $ch = curl_init($url);
                curl_setopt($ch, CURLOPT_FILE, $fp);
                curl_setopt($ch, CURLOPT_TIMEOUT, 3600);
                if (self::$cookie) {
                    curl_setopt($ch, CURLOPT_COOKIE, self::$cookie);
                }
                curl_setopt($ch, CURLOPT_SSL_VERIFYPEER, false);
                curl_setopt($ch, CURLOPT_FOLLOWLOCATION, true);
                curl_setopt($ch, CURLOPT_NOPROGRESS, true);
                curl_setopt($ch, CURLOPT_USERAGENT, self::$user_agent);

                curl_multi_add_handle($mh, $ch);
                $ch_int = (int)$ch;
                $active_transfers[$ch_int] = [
                    'handle'  => $ch, 
                    'wrapper' => $item,
                    'fp'      => $fp,
                    'slot'    => $slot_index
                ];
                $slots[$slot_index] = $ch_int;
            }

            // 2. EXECUTE
            $status = curl_multi_exec($mh, $running);
            if ($running > 0) {
                curl_multi_select($mh, 0.1);
            }

            // 3. UPDATE UI
            $current_batch_bytes = 0; // Bytes currently downloading in this loop

            foreach ($active_transfers as $ch_int => $data) {
                $slot = $data['slot'];
                $line_offset = $slot + 1;
                $label_num = $slot + 1;

                $name_parts = explode('/', $data['wrapper']['file']->name);
                $count = count($name_parts);
                $filename = ($count > 1) ? $name_parts[$count-2] . '/' . $name_parts[$count-1] : $name_parts[0];
                $downloaded = curl_getinfo($data['handle'], CURLINFO_SIZE_DOWNLOAD);
                $total = $data['wrapper']['file']->size; 
                
                // Add to current batch accumulator for the footer
                $current_batch_bytes += $downloaded;
                if (strlen($filename) > 56) $filename = '...' . substr($filename, -53);
                $filename_padded = str_pad($filename, 56);
                $bar = self::draw_progress_bar($downloaded, $total, 15);
                $size_str = self::humanFilesize($downloaded);
                
                echo "\033[{$line_offset}B\r\033[K {$label_num}: {$filename_padded} {$bar} {$size_str}\033[{$line_offset}A";
            }

            // 4. UPDATE GLOBAL FOOTER
            // Only display if we have a total calculated
            if (self::$global_bytes_total > 0) {
                // Total Processed = (Files Finished) + (Active Download Progress)
                $display_current = self::$global_bytes_processed + $current_batch_bytes;
                // Cap at total to prevent minor overflows from confusing the user
                if ($display_current > self::$global_bytes_total) $display_current = self::$global_bytes_total;
                $human_current = self::humanFilesize($display_current);
                $human_total = self::humanFilesize(self::$global_bytes_total);
                
                // Avoid division by zero
                $global_percent = (self::$global_bytes_total > 0) ? round(($display_current / self::$global_bytes_total) * 100) : 0;
                $footer_offset = $max_concurrency + 1;
                
                // Print Footer: Flush left, no label
                echo "\033[{$footer_offset}B\r\033[K{$human_current} / {$human_total} ({$global_percent}%)\033[{$footer_offset}A";
            }

            // 5. CHECK COMPLETION
            while ($info = curl_multi_info_read($mh)) {
                $ch = $info['handle'];
                $ch_id = (int)$ch;
                
                if (isset($active_transfers[$ch_id])) {
                    $transfer = $active_transfers[$ch_id];
                    $slot = $transfer['slot'];
                    $wrapper = $transfer['wrapper'];
                    $file = $wrapper['file'];
                    
                    $curl_info = curl_getinfo($ch);
                    
                    curl_multi_remove_handle($mh, $ch);
                    if (is_resource($ch)) {
                        curl_close($ch);
                    }
                    fclose($transfer['fp']);
                    unset($active_transfers[$ch_id]);
                    $slots[$slot] = null;

                    // Validation
                    $local_dest = $folder . '/' . $file->name;
                    $actual_size = file_exists($local_dest) ? filesize($local_dest) : 0;
                    $expected_size = (int)$file->size;
                    
                    $is_http_error = $curl_info['http_code'] >= 400;
                    $is_curl_error = $info['result'] !== CURLE_OK;
                    $is_size_mismatch = $actual_size !== $expected_size;

                    if ($is_http_error || $is_curl_error || $is_size_mismatch) {
                        // Fail Fast or Retry logic
                        if ($is_http_error) {
                             $fallback_queue[] = $file;
                             $line_offset = $slot + 1;
                             echo "\033[{$line_offset}B\r\033[K {$slot}: -> HTTP {$curl_info['http_code']}. Queuing fallback...\033[{$line_offset}A";
                             $processed_count++;
                        } 
                        else if ($wrapper['attempts'] < $max_retries) {
                            array_push($queue, $wrapper);
                        } 
                        else {
                            $fallback_queue[] = $file;
                            $line_offset = $slot + 1;
                            echo "\033[{$line_offset}B\r\033[K {$slot}: -> Retries exhausted. Queuing fallback...\033[{$line_offset}A";
                            $processed_count++;
                        }
                    } else {
                        // SUCCESS: Permanently add this file's full size to global stats
                        self::$global_bytes_processed += $expected_size;
                        $processed_count++;
                    }
                }
            }

        } while ($running > 0 || !empty($queue));

        // CLEANUP: 
        // 1. Move cursor down from Top (where it sits during loop) to the Global Footer line
        $lines_to_move = $max_concurrency + 1;
        echo "\033[{$lines_to_move}B";
        
        // 2. Clear the Footer line completely so it doesn't get stuck
        echo "\r\033[K"; 
        
        // 3. Show cursor (It is now sitting on the empty line below the file list)
        echo "\033[?25h";

        // 6. PROCESS FALLBACKS
        if (!empty($fallback_queue)) {
            echo " - Processing " . count($fallback_queue) . " failed downloads via stream fallback...\n";
            foreach ($fallback_queue as $file) {
                $local_dest = $folder . '/' . $file->name;
                $human_size = self::humanFilesize($file->size);
                echo "    > Streaming {$file->name} ($human_size)...\n";
                try {
                    // Note: Stream fallback handles its own progress bar
                    self::download_file_chunked_stream($file->name, $local_dest, $file->size);
                    
                    // Success on stream: Add to global stats
                    self::$global_bytes_processed += (int)$file->size;
                } catch (\Exception $e) {
                    echo "      Error: " . $e->getMessage() . "\n";
                }
            }
        }

        echo " - All large file downloads processed.\n";
    }

    /**
     * Fetches manifest chunks in parallel using a rolling queue.
     * Returns the aggregated remote files map.
     */
    private static function fetch_manifests_parallel($manifest_chunks) {
        $max_concurrency = 8;
        $max_retries = 5;
        $mh = curl_multi_init();
        $queue = [];
        
        // Filter valid chunks and prepare queue
        foreach ($manifest_chunks as $chunk) {
            if (isset($chunk->url)) {
                $queue[] = ['chunk' => $chunk, 'attempts' => 0];
            }
        }

        $active_transfers = [];
        $total_chunks = count($queue);
        $processed_count = 0;
        $remote_files_map = [];
        $all_remote_files = [];
        $failed_chunks = [];

        echo " - Fetching $total_chunks manifest chunks (Concurrency: $max_concurrency)...\n";
        do {
            // 1. REFILL
            while (count($active_transfers) < $max_concurrency && !empty($queue)) {
                $item = array_shift($queue);
                $chunk = $item['chunk'];
                $item['attempts']++;

                $ch = curl_init($chunk->url);
                curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
                curl_setopt($ch, CURLOPT_TIMEOUT, 60);
                curl_setopt($ch, CURLOPT_CONNECTTIMEOUT, 20);
                curl_setopt($ch, CURLOPT_SSL_VERIFYPEER, false);
                if (self::$cookie) {
                    curl_setopt($ch, CURLOPT_COOKIE, self::$cookie);
                }
                curl_setopt($ch, CURLOPT_USERAGENT, self::$user_agent);
                curl_multi_add_handle($mh, $ch);
                $active_transfers[(int)$ch] = ['wrapper' => $item];
            }

            // 2. EXECUTE
            curl_multi_exec($mh, $running);
            if ($running > 0) curl_multi_select($mh);

            // 3. CHECK COMPLETION
            while ($info = curl_multi_info_read($mh)) {
                $ch = $info['handle'];
                $ch_id = (int)$ch;
                
                if (isset($active_transfers[$ch_id])) {
                    $transfer = $active_transfers[$ch_id];
                    $chunk_name = $transfer['wrapper']['chunk']->name ?? 'unknown';
                    
                    $content = curl_multi_getcontent($ch);
                    $http_code = curl_getinfo($ch, CURLINFO_HTTP_CODE);
                    $error = curl_error($ch);

                    curl_multi_remove_handle($mh, $ch);
                    if (is_resource($ch)) {
                        curl_close($ch);
                    }
                    unset($active_transfers[$ch_id]);

                    if ($http_code !== 200 || $error || empty($content)) {
                        // FAILURE
                        if ($transfer['wrapper']['attempts'] < $max_retries) {
                            echo ".";
                            array_push($queue, $transfer['wrapper']);
                        } else {
                            echo "\n   - Warning: Failed to download manifest chunk $chunk_name after $max_retries attempts.\n";
                            
                            if (self::$debug) {
                                $url = $transfer['wrapper']['chunk']->url ?? 'Unknown URL';
                                echo "     [DEBUG] URL: $url\n";
                                echo "     [DEBUG] HTTP Code: $http_code\n";
                                echo "     [DEBUG] cURL Error: " . ($error ?: 'None') . "\n";
                                if ($http_code !== 200 && !empty($content)) {
                                     // Show a snippet of the body in case it contains a server error message
                                     echo "     [DEBUG] Body Preview: " . substr(strip_tags($content), 0, 200) . "\n";
                                }
                            }

                            $failed_chunks[] = $transfer['wrapper']['chunk']; // Queue for fallback
                            $processed_count++;
                        }
                    } else {
                        // SUCCESS - Process JSON immediately
                        $files = json_decode($content);
                        if (is_array($files)) {
                            foreach ($files as $file) {
                                if (!isset($file->name)) continue;
                                // Apply exclusions using static property
                                $is_excluded = false;
                                if (!empty(self::$exclude_paths)) {
                                    foreach (self::$exclude_paths as $exclude_path) {
                                        if ($file->name === $exclude_path || strpos($file->name, $exclude_path . '/') === 0) {
                                            $is_excluded = true;
                                            break;
                                        }
                                    }
                                }
                               
                                if ($is_excluded) continue;
                                $remote_files_map[$file->name] = $file;
                                // $all_remote_files[] = $file; // Uncomment if needed for debug
                            }
                        }
                        $processed_count++;
                        // Progress indicator
                        if ($processed_count % 10 === 0 || $processed_count === $total_chunks) {
                            $percent = round(($processed_count / $total_chunks) * 100);
                            echo "     Progress: $processed_count/$total_chunks ($percent%)\r";
                        }
                    }
                }
            }
        } while ($running > 0 || !empty($queue));

        if (is_resource($mh)) {
            curl_multi_close($mh);
        }

        if (!empty($failed_chunks)) {
            echo "\n - Attempting fallback stream download for " .
            count($failed_chunks) . " failed chunks...\n";
            
            foreach ($failed_chunks as $chunk) {
                // Determine temp path for this chunk
                $temp_manifest_path = sys_get_temp_dir() .
                '/' . basename($chunk->path);
                
                try {
                    echo "   > Streaming manifest: {$chunk->path}\n";
                    // Reuse existing robust stream downloader
                    self::download_file_via_stream($chunk->path, $temp_manifest_path);
                    if (file_exists($temp_manifest_path)) {
                        $content = file_get_contents($temp_manifest_path);
                        $files = json_decode($content);
                        
                        if (is_array($files)) {
                            $added_count = 0;
                            foreach ($files as $file) {
                                if (!isset($file->name)) continue;
                                // Apply exclusions
                                $is_excluded = false;
                                if (!empty(self::$exclude_paths)) {
                                    foreach (self::$exclude_paths as $exclude_path) {
                                        if ($file->name === $exclude_path || strpos($file->name, $exclude_path . '/') === 0) {
                                            $is_excluded = true;
                                            break;
                                        }
                                    }
                                }
                               
                                if ($is_excluded) continue;
                                
                                $remote_files_map[$file->name] = $file;
                                $added_count++;
                            }
                            echo "     - Recovered $added_count files from stream.\n";
                        } else {
                            echo "     - Failed to decode JSON from stream.\n";
                        }
                        
                        unlink($temp_manifest_path);
                    }
                } catch (\Exception $e) {
                    echo "     - Critical: Fallback failed for {$chunk->name}. {$e->getMessage()}\n";
                }
            }
        }

        echo "\n - Manifest download complete.\n";
        return [$remote_files_map, $all_remote_files];
    }

    /**
     * Downloads a file from the /stream-file endpoint using WpOrg\Requests.
     */
    private static function download_file_via_stream($relative_path, $destination) {
        $attempts = 0;
        $max_attempts = 3;
        $last_error = "";

        while ($attempts < $max_attempts) {
            $attempts++;
            try {
                // Use self::$siteUrl and self::$token
                $endpoint_url = self::$siteUrl . '/wp-json/disembark/v1/stream-file';
                $payload = json_encode(['token' => self::$token, 'file' => $relative_path]);
                $headers = ['Content-Type' => 'application/json'];
                if (self::$cookie) {
                    $headers['Cookie'] = self::$cookie;
                }
                $options = [
                    'verify' => false,
                    'timeout' => 1800,
                    'connect_timeout' => 30,
                    'useragent' => self::$user_agent
                 ];
                $response = \WpOrg\Requests\Requests::post($endpoint_url, $headers, $payload, $options);

                if (!$response->success) {
                    if ($response->status_code == 404 || $response->status_code == 403) {
                        throw new \Exception("Stream API failed: HTTP {$response->status_code}. Body: " . substr($response->body, 0, 200));
                    }
                    throw new \Exception("Stream API request failed with status code: {$response->status_code}. Body: " . substr($response->body, 0, 200));
                }

                if (file_put_contents($destination, $response->body) === false) {
                    throw new \Exception("Failed to write downloaded stream content to $destination.");
                }
                return;

            } catch (\Exception $e) {
                echo "\nWarning: Failed to stream $relative_path (Attempt $attempts/$max_attempts): {$e->getMessage()}\n";
                $last_error = $e->getMessage();
                if (file_exists($destination)) unlink($destination);
                if ($attempts < $max_attempts) {
                    echo "Retrying in 2 seconds...\n";
                    sleep(2);
                }
            }
        }
        throw new \Exception("Stream failed after $max_attempts attempts. Last error: $last_error");
    }

    /**
     * Downloads a database export file, trying direct download first and falling back to stream.
     */
    private static function download_db_file($relative_path, $destination) {
        $path_parts = explode('/', $relative_path);
        $encoded_parts = array_map('rawurlencode', $path_parts);
        $encoded_relative_path = implode('/', $encoded_parts);
        $direct_url = self::$siteUrl . '/' . $encoded_relative_path;

        try {
            self::download_file_direct($direct_url, $destination);
            return;
        } catch (\Exception $e) {
            echo "\nWarning: Direct download failed for '$relative_path': {$e->getMessage()}\n";
            echo "Attempting to download via stream API as fallback...\n";
        }

        try {
            self::download_file_via_stream($relative_path, $destination);
            echo "Stream API download successful for: $relative_path\n";
        } catch (\Exception $e) {
            echo "\nStream API fallback also failed: {$e->getMessage()}\n";
            throw new \Exception("Failed to download '$relative_path' using both direct and stream methods.");
        }
    }

    /**
     * Generates a local manifest of files.
     */
    private static function generate_local_manifest($dir, $skip_checksums = false) {
        $files_map = [];
        $real_root = realpath($dir);
        if ($real_root === false) return [];

        $iterator = new \RecursiveIteratorIterator(
            new \RecursiveDirectoryIterator($real_root, \RecursiveDirectoryIterator::SKIP_DOTS),
            \RecursiveIteratorIterator::SELF_FIRST
        );
        foreach ($iterator as $file) {
            if ($file->isDir()) continue;
            $path = $file->getRealPath();
            $relative_path = ltrim(substr($path, strlen($real_root)), DIRECTORY_SEPARATOR);

            // Skip sync-specific files
            if ($relative_path === 'database.sql' ||
                $relative_path === 'db_export.sql' ||
                strpos($relative_path, '.disembark-db-parts') === 0 ||
                strpos($relative_path, '.disembark-manifests') === 0) {
                continue;
            }

            // Skip expensive MD5 calc if requested
            $checksum = ($skip_checksums) ? null : md5_file($path);

            $files_map[$relative_path] = (object) [
                'name' => $relative_path,
                'size' => $file->getSize(),
                'checksum' => $checksum
            ];
        }
        return $files_map;
    }

    /**
     * Recursively deletes empty directories.
     */
    private static function delete_empty_dirs($dir) {
        if (!is_dir($dir)) return;
        $iterator = new \RecursiveIteratorIterator(new \RecursiveDirectoryIterator($dir, \RecursiveDirectoryIterator::SKIP_DOTS), \RecursiveIteratorIterator::CHILD_FIRST);
        foreach ($iterator as $file) {
            if ($file->isDir() && !(new \FilesystemIterator($file->getPathname()))->valid()) {
                rmdir($file->getPathname());
            }
        }
    }

    /**
     * Unzips a file to a destination.
     */
    private static function unzip_file($file, $destination) {
        if (!class_exists('ZipArchive')) {
            throw new \Exception('ZipArchive PHP extension is required to unpack files.');
        }
        
        $zip = new \ZipArchive;
        $res = $zip->open($file);
        
        if ($res === TRUE) {
            if ($zip->extractTo($destination) === FALSE) {
                $zip->close();
                throw new \Exception("Failed to extract $file to $destination.");
            }
            $zip->close();
        } else {
            // $res contains the error code (e.g. ZipArchive::ER_NOZIP)
            throw new \Exception("Failed to open zip archive $file. Error code: $res");
        }
    }

    /**
     * Zips a directory into a single archive.
     */
    private static function zip_directory($source, $destination) {
        if (!class_exists('ZipArchive')) {
            throw new \Exception('ZipArchive PHP extension is required to create the final zip.');
        }
        $zip = new \ZipArchive();
        if (!$zip->open($destination, \ZipArchive::CREATE | \ZipArchive::OVERWRITE)) {
            throw new \Exception("Failed to create zip archive at $destination.");
        }

        $source = realpath($source);
        if ($source === false) {
            throw new \Exception("Source directory $source does not exist.");
        }

        $files = new \RecursiveIteratorIterator(
            new \RecursiveDirectoryIterator($source, \RecursiveDirectoryIterator::SKIP_DOTS),
            \RecursiveIteratorIterator::SELF_FIRST
        );
        // Add the root 'public' directory itself
        $zip->addEmptyDir('public');
        foreach ($files as $file) {
            $file = realpath($file);
            // Get relative path for root of zip
            $relativePath = 'public' . DIRECTORY_SEPARATOR . substr($file, strlen($source) + 1);

            if (empty($relativePath)) continue; // Skip the root folder itself

            if (is_dir($file)) {
                $zip->addEmptyDir($relativePath);
            } else if (is_file($file)) {
                $zip->addFile($file, $relativePath);
            }
        }
        $zip->close();
    }

    /**
     * Recursively deletes a directory.
     */
    private static function delete_directory($dir) {
        if (!is_dir($dir)) {
            return;
        }
        $it = new \RecursiveDirectoryIterator($dir, \RecursiveDirectoryIterator::SKIP_DOTS);
        $files = new \RecursiveIteratorIterator($it, \RecursiveIteratorIterator::CHILD_FIRST);
        foreach ($files as $file) {
            if ($file->isDir()) {
                rmdir($file->getRealPath());
            } else {
                unlink($file->getRealPath());
            }
        }
        rmdir($dir);
    }
    // liveExecuteCommand is (no longer used by backup)
    private static function liveExecuteCommand($cmd) {
        while (@ ob_end_flush());
        $proc = popen("$cmd 2>&1", 'r');
        if ($proc === false) {
            echo "Error: Failed to execute command: $cmd\n";
            return;
        }
        $live_output = "";
        $complete_output = "";
        while (!feof($proc)) {
            $live_output = fread($proc, 4096);
            if ($live_output === false) {
                break;
            }
            $complete_output = $complete_output . $live_output;
            echo "$live_output";
            @ flush();
        }

        pclose($proc);
    }
}

// Check if the script is being run directly from the command line
if (php_sapi_name() == 'cli') {
    // Ensure vendor autoload is required relative to this script's directory
    require_once __DIR__ . '/vendor/autoload.php';
    \Disembark\Run::main($argv, $argc);
}